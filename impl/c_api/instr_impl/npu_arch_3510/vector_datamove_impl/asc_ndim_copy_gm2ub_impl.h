/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

/* !
 * \file asc_ndim_copy_gm2ub_impl.h
 * \brief
 */
#ifndef IMPL_C_API_INSTR_IMPL_NPU_ARCH_3510_VECTOR_DATAMOVE_IMPL_ASC_NDIM_COPY_GM2UB_IMPL_H
#define IMPL_C_API_INSTR_IMPL_NPU_ARCH_3510_VECTOR_DATAMOVE_IMPL_ASC_NDIM_COPY_GM2UB_IMPL_H

#include "instr_impl/npu_arch_3510/utils_impl.h"

//===========asc_ndim_copy_gm2ub(int8/uint8/e2m1/e1m2/e8m0/e5m2/e4m3fn/int16/uint16/half/bfloat16/int32/uint32/float)===========
__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ int8_t* dst, __gm__ int8_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b8((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ uint8_t* dst, __gm__ uint8_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b8((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ fp4x2_e2m1_t* dst, __gm__ fp4x2_e2m1_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b8((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ fp4x2_e1m2_t* dst, __gm__ fp4x2_e1m2_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b8((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ fp8_e8m0_t* dst, __gm__ fp8_e8m0_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b8((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ fp8_e5m2_t* dst, __gm__ fp8_e5m2_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b8((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ fp8_e4m3fn_t* dst, __gm__ fp8_e4m3fn_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b8((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ int16_t* dst, __gm__ int16_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b16((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ uint16_t* dst, __gm__ uint16_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b16((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ half* dst, __gm__ half* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b16((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ bfloat16_t* dst, __gm__ bfloat16_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b16((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ int32_t* dst, __gm__ int32_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b32((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ uint32_t* dst, __gm__ uint32_t* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b32((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

__aicore__ inline void asc_ndim_copy_gm2ub_impl(__ubuf__ float* dst, __gm__ float* src,
    uint32_t loop0_size, uint32_t loop1_size, uint32_t loop2_size, uint32_t loop3_size, uint32_t loop4_size,
    uint8_t loop0_lp_count, uint8_t loop0_rp_count, bool padding_mode, uint8_t cache_mode)
{
    if ASC_IS_AIV {
        nddma_out_to_ub_b32((__ubuf__ void*)dst, (__gm__ void*)src, 0, loop0_size, loop1_size, loop2_size,
            loop3_size, loop4_size, loop0_lp_count, loop0_rp_count, padding_mode, cache_mode);
    }
}

#endif