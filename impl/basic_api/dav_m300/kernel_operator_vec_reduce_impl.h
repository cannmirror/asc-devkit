/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

/*!
 * \file kernel_operator_vec_reduce_impl.h
 * \brief
 */
#ifndef ASCENDC_MODULE_OPERATOR_VEC_REDUCE_IMPL_H
#define ASCENDC_MODULE_OPERATOR_VEC_REDUCE_IMPL_H

#include "kernel_common.h"
#include "kernel_tpipe.h"
#include "kernel_operator_block_sync_intf.h"

namespace AscendC {
#define VCPADD_FUNC() vcpadd(vreg1, vreg0, preg, MODE_ZEROING)
#define VCGADD_FUNC() vcgadd(vreg1, vreg0, preg, MODE_ZEROING)
#define VCGMAX_FUNC() vcgmax(vreg1, vreg0, preg, MODE_ZEROING)
#define VCGMIN_FUNC() vcgmin(vreg1, vreg0, preg, MODE_ZEROING)
#define VCMAX_FUNC() vcmax(vreg1, vreg0, preg, MODE_ZEROING)
#define VCMIN_FUNC() vcmin(vreg1, vreg0, preg, MODE_ZEROING)
#define VCADD_FUNC() vcadd(vreg1, vreg0, preg, MODE_ZEROING)

#define CONTINUOUS_MODE_REDUCE_VF(reduceFunc, vregType, pltType, dstStrideOffset) \
    __VEC_SCOPE__                                                                 \
    {                                                                             \
        vector_##vregType vreg0;                                                  \
        vector_##vregType vreg1;                                                  \
        vector_align ureg;                                                        \
        uint32_t sreg = static_cast<uint32_t>(mask);                                           \
        vector_bool preg = plt_##pltType(sreg, POST_UPDATE);                      \
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);                 \
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {                      \
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);       \
            reduceFunc();                                                         \
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);                \
            vstas(ureg, dst, dstStrideOffset*(newDstRepStride - 1), POST_UPDATE); \
        }                                                                         \
    }

#define BITBYBIT_MODE_HALF_REDUCE_VF(reduceFunc, dstStrideOffset)                 \
    __VEC_SCOPE__                                                                 \
    {                                                                             \
        vector_f16 vreg0;                                                         \
        vector_f16 vreg1;                                                         \
        vector_align ureg;                                                        \
        vector_bool preg;                                                         \
        preg = movp_b16();                                                        \
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);                 \
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {                      \
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);       \
            reduceFunc();                                                         \
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);                \
            vstas(ureg, dst, dstStrideOffset*(newDstRepStride - 1), POST_UPDATE); \
        }                                                                         \
    }

#define BITBYBIT_MODE_FLOAT_REDUCE_VF(reduceFunc, dstStrideOffset)                \
    __VEC_SCOPE__                                                                 \
    {                                                                             \
        vector_f32 vreg0;                                                         \
        vector_f32 vreg1;                                                         \
        vector_align ureg;                                                        \
        vector_bool preg;                                                         \
        preg = movp_b32();                                                        \
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);                 \
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {                      \
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);       \
            reduceFunc();                                                         \
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);                \
            vstas(ureg, dst, dstStrideOffset*(newDstRepStride - 1), POST_UPDATE); \
        }                                                                         \
    }

/* **************************************** Pair Reduce Impl ****************************************** */
template <typename T, bool isSetMask = true>
__aicore__ inline void PairReduceSumImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime, const int32_t mask,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T, bool isSetMask = true>
__aicore__ inline void PairReduceSumImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime, const uint64_t mask[],
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void PairReduceSumImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_HALF_REDUCE_VF(VCPADD_FUNC, FULL_MASK_LEN / HALF_FACTOR);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void PairReduceSumImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_FLOAT_REDUCE_VF(VCPADD_FUNC, HLAF_MASK_LEN / HALF_FACTOR);
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void PairReduceSumImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCPADD_FUNC, f16, b16, FULL_MASK_LEN / HALF_FACTOR);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void PairReduceSumImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCPADD_FUNC, f32, b32, HLAF_MASK_LEN / HALF_FACTOR);
}

/* **************************************** Block Reduce Impl ****************************************** */
template <typename T, bool isSetMask = true>
__aicore__ inline void BlockReduceSumImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}
template <typename T, bool isSetMask = true>
__aicore__ inline void BlockReduceSumImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime, const int32_t mask,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void BlockReduceSumImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_HALF_REDUCE_VF(VCGADD_FUNC, DEFAULT_BLK_NUM);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void BlockReduceSumImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_FLOAT_REDUCE_VF(VCGADD_FUNC, DEFAULT_BLK_NUM);
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void BlockReduceSumImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCGADD_FUNC, f16, b16, DEFAULT_BLK_NUM);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void BlockReduceSumImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCGADD_FUNC, f32, b32, DEFAULT_BLK_NUM);
}

template <typename T, bool isSetMask = true>
__aicore__ inline void BlockReduceMaxImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T, bool isSetMask = true>
__aicore__ inline void BlockReduceMaxImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime, const int32_t mask,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void BlockReduceMaxImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_HALF_REDUCE_VF(VCGMAX_FUNC, DEFAULT_BLK_NUM);
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void BlockReduceMaxImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCGMAX_FUNC, f16, b16, DEFAULT_BLK_NUM);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void BlockReduceMaxImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_FLOAT_REDUCE_VF(VCGMAX_FUNC, DEFAULT_BLK_NUM);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void BlockReduceMaxImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCGMAX_FUNC, f32, b32, DEFAULT_BLK_NUM);
}

template <typename T, bool isSetMask = true>
__aicore__ inline void BlockReduceMinImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T, bool isSetMask = true>
__aicore__ inline void BlockReduceMinImpl(__ubuf__ T* dst, __ubuf__ T* src, const int32_t repeatTime, const int32_t mask,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void BlockReduceMinImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_HALF_REDUCE_VF(VCGMIN_FUNC, DEFAULT_BLK_NUM);
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void BlockReduceMinImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCGMIN_FUNC, f16, b16, DEFAULT_BLK_NUM);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void BlockReduceMinImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_FLOAT_REDUCE_VF(VCGMIN_FUNC, DEFAULT_BLK_NUM);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void BlockReduceMinImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCGMIN_FUNC, f32, b32, DEFAULT_BLK_NUM);
}

template <typename T, bool isSetMask = true>
__aicore__ inline void RepeatReduceSumImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, const int32_t repeatTime,
    const int32_t elemsInOneRepeat, const int32_t dstBlkStride, const int32_t srcBlkStride, const int32_t dstRepStride,
    const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

/* **************************************** Whole Reduce Interface ****************************************** */
template <typename T, bool isSetMask = true>
__aicore__ inline void WholeReduceMaxImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T, bool isSetMask = true>
__aicore__ inline void WholeReduceMaxImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void WholeReduceMaxImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b16(sreg, POST_UPDATE);
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmax(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void WholeReduceMaxImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b32(sreg, POST_UPDATE);
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmax(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void WholeReduceMaxImpl(__ubuf__ half* dst, __ubuf__ half* src, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        vector_bool preg;
        preg = movp_b16();
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmax(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void WholeReduceMaxImpl(__ubuf__ float* dst, __ubuf__ float* src, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        vector_bool preg;
        preg = movp_b32();
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmax(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T, bool isSetMask = true>
__aicore__ inline void WholeReduceMinImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T, bool isSetMask = true>
__aicore__ inline void WholeReduceMinImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void WholeReduceMinImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b16(sreg, POST_UPDATE);
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmin(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void WholeReduceMinImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b32(sreg, POST_UPDATE);
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmin(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void WholeReduceMinImpl(__ubuf__ half* dst, __ubuf__ half* src, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }

    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        vector_bool preg;
        preg = movp_b16();
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmin(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void WholeReduceMinImpl(__ubuf__ float* dst, __ubuf__ float* src, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t dstStrideOffset = (order == ReduceOrder::ORDER_VALUE_INDEX) ? 2 : 1;
    uint32_t dstStrideOffsetPost = dstStrideOffset * (dstRepStride - 1);
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
    }
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        vector_bool preg;
        preg = movp_b32();
        uint32_t strideConfig = ((static_cast<uint32_t>(srcBlkStride)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(newRepeat); ++i) {
            vsldb(vreg0, newSrc + i * srcStrideOffset, strideConfig, preg);
            vcmin(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, dstStrideOffset, vreg1, dst, POST_UPDATE);
            vstas(ureg, dst, dstStrideOffsetPost, POST_UPDATE);
        }
    }
}

template <typename T, bool isSetMask = true>
__aicore__ inline void WholeReduceSumImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T, bool isSetMask = true>
__aicore__ inline void WholeReduceSumImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "current data type is not supported!"); });
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void WholeReduceSumImpl(__ubuf__ half* dst, __ubuf__ half* src, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCADD_FUNC, f16, b16, 1);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void WholeReduceSumImpl(__ubuf__ float* dst, __ubuf__ float* src, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    CONTINUOUS_MODE_REDUCE_VF(VCADD_FUNC, f32, b32, 1);
}

template <typename T = half, bool isSetMask = true>
__aicore__ inline void WholeReduceSumImpl(__ubuf__ half* dst, __ubuf__ half* src, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }

    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ half* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_HALF_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_HALF_REDUCE_VF(VCADD_FUNC, 1);
}

template <typename T = float, bool isSetMask = true>
__aicore__ inline void WholeReduceSumImpl(__ubuf__ float* dst, __ubuf__ float* src, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr (isSetMask) {
        SetVectorMask<T>(mask[1], mask[0]);
    }
    int32_t newRepeat = repeatTime;
    uint32_t srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM;
    uint32_t newDstRepStride = dstRepStride;
    __ubuf__ float* newSrc = src;
    if (dstRepStride == 0 && repeatTime > 0) {
        newRepeat = 1;
        srcStrideOffset = srcRepStride * ONE_BLK_FLOAT_NUM * (repeatTime - 1);
        newSrc += srcStrideOffset;
        newDstRepStride = 1;
    }
    BITBYBIT_MODE_FLOAT_REDUCE_VF(VCADD_FUNC, 1);
}

/* **************************************** Reduce Interface ****************************************** */
template <typename T>
__aicore__ inline void ReduceMaxIntrinsicsImpl(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "ReduceMaxIntrinsicsImpl is not supported!"); });
}

template <typename T>
__aicore__ inline void ReduceMaxIntrinsicsImpl(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "ReduceMaxIntrinsicsImpl is not supported!"); });
}

__aicore__ inline void ReduceMaxIntrinsicsImpl(__ubuf__ half* sharedTmpBuffer, __ubuf__ half* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b16(sreg, POST_UPDATE);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 16, strideConfig0, preg);
            vcmax(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
}

__aicore__ inline void ReduceMaxIntrinsicsImpl(__ubuf__ half* sharedTmpBuffer, __ubuf__ half* srcLocal,
    const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride)
{
    __ubuf__ uint8_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint8_t>(TMP_UB_OFFSET, 16);
    *((__ubuf__ uint64_t*)tempBuf) = mask[0];
    *((__ubuf__ uint64_t*)tempBuf + 1) = mask[1];

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        vector_bool preg;
        plds(preg, ((__ubuf__ uint32_t*)tempBuf), 0, US);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 16, strideConfig0, preg);
            vcmax(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
    AscendCUtils::FreeTemporaryBuffer<uint8_t>(tempBuf);
}

__aicore__ inline void ReduceMaxIntrinsicsImpl(__ubuf__ float* sharedTmpBuffer, __ubuf__ float* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b32(sreg, POST_UPDATE);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 8, strideConfig0, preg);
            vcmax(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
}

__aicore__ inline void ReduceMaxIntrinsicsImpl(__ubuf__ float* sharedTmpBuffer, __ubuf__ float* srcLocal,
    const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride)
{
    __ubuf__ uint8_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint8_t>(TMP_UB_OFFSET, 16);
    *((__ubuf__ uint64_t*)tempBuf) = mask[0];
    *((__ubuf__ uint64_t*)tempBuf + 1) = mask[1];

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        vector_bool preg;
        vector_bool preg1;
        plds(preg, ((__ubuf__ uint32_t*)tempBuf), 0, US);
        punpack(preg1, preg, LOWER);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 8, strideConfig0, preg1);
            vcmax(vreg1, vreg0, preg1, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
    AscendCUtils::FreeTemporaryBuffer<uint8_t>(tempBuf);
}

template <typename T>
__aicore__ inline void ReduceMinIntrinsicsImpl(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "ReduceMinIntrinsicsImpl is not supported!"); });
}

template <typename T>
__aicore__ inline void ReduceMinIntrinsicsImpl(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "ReduceMinIntrinsicsImpl is not supported!"); });
}

__aicore__ inline void ReduceMinIntrinsicsImpl(__ubuf__ half* sharedTmpBuffer, __ubuf__ half* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b16(sreg, POST_UPDATE);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 16, strideConfig0, preg);
            vcmin(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
}

__aicore__ inline void ReduceMinIntrinsicsImpl(__ubuf__ half* sharedTmpBuffer, __ubuf__ half* srcLocal,
    const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride)
{
    __ubuf__ uint8_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint8_t>(TMP_UB_OFFSET, 16);
    *((__ubuf__ uint64_t*)tempBuf) = mask[0];
    *((__ubuf__ uint64_t*)tempBuf + 1) = mask[1];

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        vector_bool preg;
        plds(preg, ((__ubuf__ uint32_t*)tempBuf), 0, US);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 16, strideConfig0, preg);
            vcmin(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
    AscendCUtils::FreeTemporaryBuffer<uint8_t>(tempBuf);
}

__aicore__ inline void ReduceMinIntrinsicsImpl(__ubuf__ float* sharedTmpBuffer, __ubuf__ float* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b32(sreg, POST_UPDATE);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 8, strideConfig0, preg);
            vcmin(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
}

__aicore__ inline void ReduceMinIntrinsicsImpl(__ubuf__ float* sharedTmpBuffer, __ubuf__ float* srcLocal,
    const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride)
{
    __ubuf__ uint8_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint8_t>(TMP_UB_OFFSET, 16);
    *((__ubuf__ uint64_t*)tempBuf) = mask[0];
    *((__ubuf__ uint64_t*)tempBuf + 1) = mask[1];

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        vector_bool preg;
        vector_bool preg1;
        plds(preg, ((__ubuf__ uint32_t*)tempBuf), 0, US);
        punpack(preg1, preg, LOWER);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 8, strideConfig0, preg1);
            vcmin(vreg1, vreg0, preg1, MODE_ZEROING);
            vstus(ureg, 2, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
    AscendCUtils::FreeTemporaryBuffer<uint8_t>(tempBuf);
}

template <typename T>
__aicore__ inline void ReduceSumIntrinsicsImpl(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "ReduceSumIntrinsicsImpl is not supported!"); });
}

template <typename T>
__aicore__ inline void ReduceSumIntrinsicsImpl(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t srcRepStride)
{
    ASCENDC_ASSERT(false, { KERNEL_LOG(KERNEL_ERROR, "ReduceSumIntrinsicsImpl is not supported!"); });
}

__aicore__ inline void ReduceSumIntrinsicsImpl(__ubuf__ half* sharedTmpBuffer, __ubuf__ half* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b16(sreg, POST_UPDATE);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 16, strideConfig0, preg);
            vcadd(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 1, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
}

__aicore__ inline void ReduceSumIntrinsicsImpl(__ubuf__ half* sharedTmpBuffer, __ubuf__ half* srcLocal,
    const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride)
{
    __ubuf__ uint8_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint8_t>(TMP_UB_OFFSET, 16);
    *((__ubuf__ uint64_t*)tempBuf) = mask[0];
    *((__ubuf__ uint64_t*)tempBuf + 1) = mask[1];

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    __VEC_SCOPE__
    {
        vector_f16 vreg0;
        vector_f16 vreg1;
        vector_align ureg;
        vector_bool preg;
        plds(preg, ((__ubuf__ uint32_t*)tempBuf), 0, US);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 16, strideConfig0, preg);
            vcadd(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 1, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
    AscendCUtils::FreeTemporaryBuffer<uint8_t>(tempBuf);
}

__aicore__ inline void ReduceSumIntrinsicsImpl(__ubuf__ float* sharedTmpBuffer, __ubuf__ float* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t srcRepStride)
{
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        uint32_t sreg = static_cast<uint32_t>(mask);
        vector_bool preg = plt_b32(sreg, POST_UPDATE);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 8, strideConfig0, preg);
            vcadd(vreg1, vreg0, preg, MODE_ZEROING);
            vstus(ureg, 1, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
}

__aicore__ inline void ReduceSumIntrinsicsImpl(__ubuf__ float* sharedTmpBuffer, __ubuf__ float* srcLocal,
    const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride)
{
    __ubuf__ uint8_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint8_t>(TMP_UB_OFFSET, 16);
    *((__ubuf__ uint64_t*)tempBuf) = mask[0];
    *((__ubuf__ uint64_t*)tempBuf + 1) = mask[1];

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    __VEC_SCOPE__
    {
        vector_f32 vreg0;
        vector_f32 vreg1;
        vector_align ureg;
        vector_bool preg;
        vector_bool preg1;
        plds(preg, ((__ubuf__ uint32_t*)tempBuf), 0, US);
        punpack(preg1, preg, LOWER);
        uint32_t strideConfig0 = ((static_cast<uint32_t>(1)) << 16);
        uint32_t strideConfig1 = ((static_cast<uint32_t>(0)) << 16);
        for (uint16_t i = 0; i < static_cast<uint16_t>(repeatTime); ++i) {
            vsldb(vreg0, srcLocal + i * srcRepStride * 8, strideConfig0, preg1);
            vcadd(vreg1, vreg0, preg1, MODE_ZEROING);
            vstus(ureg, 1, vreg1, sharedTmpBuffer, POST_UPDATE);
        }
        vstas(ureg, sharedTmpBuffer, 0, POST_UPDATE);
    }
    AscendCUtils::FreeTemporaryBuffer<uint8_t>(tempBuf);
}

template <typename T>
__aicore__ inline void ReduceSumSecondStep(__ubuf__ T* dstLocal, __ubuf__ T* sharedTmpBuffer,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T); // fp16=128 , fp32=64
    int32_t newRepeatTimes = params.repeatTimes / elementNumPerRep;
    int32_t leftData = params.repeatTimes % elementNumPerRep;

    if (newRepeatTimes != 0) {
        ReduceSumIntrinsicsImpl(sharedTmpBuffer, sharedTmpBuffer, elementNumPerRep, newRepeatTimes, DEFAULT_REPEAT_STRIDE);
    }

    if (leftData > 0) { // has_tail
        srcOffset = elementNumPerRep * newRepeatTimes;
        ReduceSumIntrinsicsImpl(dstLocal, sharedTmpBuffer + srcOffset, leftData, 1, DEFAULT_REPEAT_STRIDE);
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *(sharedTmpBuffer + newRepeatTimes) = *dstLocal;
        if (newRepeatTimes != 0) {
            event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
            SetFlag<HardEvent::S_V>(eventIdSToV);
            WaitFlag<HardEvent::S_V>(eventIdSToV);
        }
    }
}

template <typename T>
__aicore__ inline void CreateSpecialFormatMask(const int32_t& maskLen, uint64_t& highMask, uint64_t& lowMask)
{
    // create mask in the "0101010101" format
    int32_t halfLen = HLAF_MASK_LEN / 2;
    for (int32_t i = 0; i < maskLen - halfLen; i++) {
        highMask = highMask << 2;
        highMask = highMask | 1;
    }
    int32_t lowMaskRange = maskLen >= halfLen ? halfLen : maskLen;
    for (int32_t i = 0; i < lowMaskRange; i++) {
        lowMask = lowMask << 2;
        lowMask = lowMask | 1;
    }
}

template <typename T>
__aicore__ inline void ReduceOperation(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal, struct ReduceRepeatParams& params,
    const ReduceMode& mode)
{
    if (params.maskMode == 1) {
        switch (mode) {
            case ReduceMode::REDUCE_MAX:
                ReduceMaxIntrinsicsImpl(sharedTmpBuffer, srcLocal, params.normalMask, params.repeatTimes,
                    params.srcRepStride);
                break;
            case ReduceMode::REDUCE_MIN:
                ReduceMinIntrinsicsImpl(sharedTmpBuffer, srcLocal, params.normalMask, params.repeatTimes,
                    params.srcRepStride);
                break;
            case ReduceMode::REDUCE_SUM:
                ReduceSumIntrinsicsImpl(sharedTmpBuffer, srcLocal, params.normalMask, params.repeatTimes,
                    params.srcRepStride);
                break;
            default:
                break;
        }
    } else {
        switch (mode) {
            case ReduceMode::REDUCE_MAX:
                ReduceMaxIntrinsicsImpl(sharedTmpBuffer, srcLocal, params.bitMask, params.repeatTimes, params.srcRepStride);
                break;
            case ReduceMode::REDUCE_MIN:
                ReduceMinIntrinsicsImpl(sharedTmpBuffer, srcLocal, params.bitMask, params.repeatTimes, params.srcRepStride);
                break;
            case ReduceMode::REDUCE_SUM:
                ReduceSumIntrinsicsImpl(sharedTmpBuffer, srcLocal, params.bitMask, params.repeatTimes, params.srcRepStride);
                break;
            default:
                break;
        }
    }
}

template <typename T>
__aicore__ inline void ReduceImplFirstStep(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode, int32_t& curData)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t range = params.repeatTimes / MAX_REPEAT_TIMES;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = index * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        struct ReduceRepeatParams newParams = params;
        newParams.repeatTimes = MAX_REPEAT_TIMES;
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, newParams, mode);
    }
    int32_t leftRepeatTimes = params.repeatTimes % MAX_REPEAT_TIMES;
    if (leftRepeatTimes > 0) {
        dstOffset = range * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = range * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        struct ReduceRepeatParams leftParams = params;
        leftParams.repeatTimes = leftRepeatTimes;
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, leftParams, mode);
    }
    curData = VREDUCE_PER_REP_OUTPUT * params.repeatTimes;
}

template <typename T>
__aicore__ inline void ReduceImplSecondStep(__ubuf__ T* sharedTmpBuffer, const ReduceMode& mode, int32_t& curData,
    int32_t preStartPos, int32_t secondStartPos)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t newMaskLen = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0, lowMask = 0;
    uint64_t newMask[2];
    int32_t bodyOutputCount = 0;
    int32_t tailOutputCount = 0;

    if (newRepeatTimes >= 1) {
        highMask = (sizeof(T) == sizeof(half)) ? 0x5555555555555555 : 0;
        lowMask = 0x5555555555555555;
        newMask[0] = lowMask;
        newMask[1] = highMask;
        struct ReduceRepeatParams newParams(newMask, newRepeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);

        ReduceOperation<T>(sharedTmpBuffer + secondStartPos, sharedTmpBuffer + preStartPos, newParams, mode);
        bodyOutputCount = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
    }
    highMask = 0;
    lowMask = 0;

    if (leftData > 0) {
        newMaskLen = leftData / VREDUCE_PER_REP_OUTPUT;
        // create mask in the "0101010101" format
        CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);
        newMask[0] = lowMask;
        newMask[1] = highMask;
        struct ReduceRepeatParams leftParams(newMask, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);

        dstOffset = secondStartPos + bodyOutputCount;
        srcOffset = preStartPos + newRepeatTimes * elementNumPerRep;
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, sharedTmpBuffer + srcOffset, leftParams, mode);
        tailOutputCount = VREDUCE_PER_REP_OUTPUT;
    }

    curData = bodyOutputCount + tailOutputCount;
}

template <typename T>
__aicore__ inline void GetIndex(__ubuf__ T* sharedTmpBuffer, int32_t secondStartPos, int32_t& secondIndex,
    int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        thirdIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer + secondStartPos + 1);
        ASSERT(thirdIndex >= 0);
        ASSERT(thirdIndex < elementNumPerRep);
        secondIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer + thirdIndex + 1);
        ASSERT(secondIndex >= 0);
        ASSERT(secondIndex < elementNumPerRep);
    } else {
        thirdIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer + secondStartPos + 1);
        ASSERT(thirdIndex >= 0);
        ASSERT(thirdIndex < elementNumPerRep);
        secondIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer + thirdIndex + 1);
        ASSERT(secondIndex >= 0);
        ASSERT(secondIndex < elementNumPerRep);
    }
}

template <typename T>
__aicore__ inline void GetIndex(__ubuf__ T* sharedTmpBuffer, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        thirdIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer + thirdStartPos + 1);
        ASSERT(thirdIndex >= 0);
        ASSERT(thirdIndex < elementNumPerRep);
        secondIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer + secondStartPos + thirdIndex + 1);
        ASSERT(secondIndex >= 0);
        ASSERT(secondIndex < elementNumPerRep);
        firstIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer +
            elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex + 1);
        ASSERT(firstIndex >= 0);
        ASSERT(firstIndex < elementNumPerRep);
    } else {
        thirdIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer + thirdStartPos + 1);
        ASSERT(thirdIndex >= 0);
        ASSERT(thirdIndex < elementNumPerRep);
        secondIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer + secondStartPos + thirdIndex + 1);
        ASSERT(secondIndex >= 0);
        ASSERT(secondIndex < elementNumPerRep);
        firstIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer +
            elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex + 1);
        ASSERT(firstIndex >= 0);
        ASSERT(firstIndex < elementNumPerRep);
    }
}

template <typename T>
__aicore__ inline void GetIndex(__ubuf__ T* sharedTmpBuffer, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t fourthStartPos, int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex, int32_t& fourthIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        fourthIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer + fourthStartPos + 1);
        ASSERT(fourthIndex >= 0);
        ASSERT(fourthIndex < elementNumPerRep);
        thirdIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer + thirdStartPos + fourthIndex + 1);
        ASSERT(thirdIndex >= 0);
        ASSERT(thirdIndex < elementNumPerRep);
        secondIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);
        ASSERT(secondIndex >= 0);
        ASSERT(secondIndex < elementNumPerRep);
        firstIndex = *reinterpret_cast<__ubuf__ uint16_t*>(sharedTmpBuffer +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);
        ASSERT(firstIndex >= 0);
        ASSERT(firstIndex < elementNumPerRep);
    } else {
        fourthIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer + fourthStartPos + 1);
        ASSERT(fourthIndex >= 0);
        ASSERT(fourthIndex < elementNumPerRep);
        thirdIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer + thirdStartPos + fourthIndex + 1);
        ASSERT(thirdIndex >= 0);
        ASSERT(thirdIndex < elementNumPerRep);
        secondIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);
        ASSERT(secondIndex >= 0);
        ASSERT(secondIndex < elementNumPerRep);
        firstIndex = *reinterpret_cast<__ubuf__ uint32_t*>(sharedTmpBuffer +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);
        ASSERT(firstIndex >= 0);
        ASSERT(firstIndex < elementNumPerRep);
    }
}

template <typename T>
__aicore__ inline void ReduceImplThirdStep(__ubuf__ T* dstLocal, __ubuf__ T* sharedTmpBuffer, const int32_t srcRepStride,
    const ReduceMode& mode, int32_t& curData, int32_t& secondStartPos, int32_t& thirdStartPos)
{
    int32_t preNum = 0;
    int32_t firstIndex = 0;
    int32_t secondIndex = 0;
    int32_t thirdIndex = 0;
    int32_t fourthIndex = 0;
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    uint64_t newMask[2];
    int32_t offsetNumPerRep = ONE_BLK_SIZE / sizeof(T) * srcRepStride;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    if (curData == VREDUCE_PER_REP_OUTPUT) {
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        GetIndex<T>(sharedTmpBuffer, secondStartPos, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT);
        int32_t resultIndex = secondIndex + preNum;
        *dstLocal = *(sharedTmpBuffer + secondStartPos);
        *(dstLocal + 1) = *reinterpret_cast<T*>(&resultIndex);
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        return;
    }

    int32_t newMaskLen = curData / VREDUCE_PER_REP_OUTPUT;
    CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);
    newMask[0] = lowMask;
    newMask[1] = highMask;
    if (curData > elementNumPerRep) {
        ReduceImplSecondStep<T>(sharedTmpBuffer, mode, curData, secondStartPos, thirdStartPos);

        int32_t fourthStartPos =
            (((thirdStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
        dstOffset = fourthStartPos;
        srcOffset = thirdStartPos;
        struct ReduceRepeatParams newParams(newMask, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);

        ReduceOperation<T>(sharedTmpBuffer + dstOffset, sharedTmpBuffer + srcOffset, newParams, mode);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *dstLocal = *(sharedTmpBuffer + dstOffset);

        GetIndex<T>(sharedTmpBuffer, secondStartPos, thirdStartPos, fourthStartPos, firstIndex, secondIndex, thirdIndex,
            fourthIndex);
        preNum = offsetNumPerRep *
            (elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex) /
            VREDUCE_PER_REP_OUTPUT;
    } else {
        dstOffset = thirdStartPos;
        srcOffset = secondStartPos;
        struct ReduceRepeatParams newParams(newMask, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, sharedTmpBuffer + srcOffset, newParams, mode);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *dstLocal = *(sharedTmpBuffer + thirdStartPos);

        GetIndex<T>(sharedTmpBuffer, secondStartPos, thirdStartPos, firstIndex, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex) /
            VREDUCE_PER_REP_OUTPUT;
    }

    int32_t resultIndex = firstIndex + preNum;
    *(dstLocal + 1) = *reinterpret_cast<T*>(&resultIndex);
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
__aicore__ inline void ReduceSumFirstStep(__ubuf__ T* sharedTmpBuffer, __ubuf__ T* srcLocal,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t maxRepeatTimes = (MAX_REPEAT_TIMES - ONE_BLK_SIZE / sizeof(T) + 1);
    int32_t range = params.repeatTimes / maxRepeatTimes;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * maxRepeatTimes;
        srcOffset = index * maxRepeatTimes * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        struct ReduceRepeatParams newParams = params;
        newParams.repeatTimes = maxRepeatTimes;
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, newParams, ReduceMode::REDUCE_SUM);
    }

    int32_t leftRepeatTimes = params.repeatTimes % maxRepeatTimes;
    if (leftRepeatTimes > 0) {
        dstOffset = range * maxRepeatTimes;
        srcOffset = range * maxRepeatTimes * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        struct ReduceRepeatParams leftParams = params;
        leftParams.repeatTimes = leftRepeatTimes;
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, leftParams, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
__aicore__ inline void ReduceSumFinalStep(__ubuf__ T* dstLocal, __ubuf__ T* sharedTmpBuffer, int32_t& secondResultNum)
{
    if (secondResultNum == 1) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *(dstLocal) = *(sharedTmpBuffer);
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    } else {
        struct ReduceRepeatParams newParams(secondResultNum, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
        ReduceOperation<T>(dstLocal, sharedTmpBuffer, newParams, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
__aicore__ inline void ReduceSumImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, __ubuf__ T* sharedTmpBuffer,
    struct ReduceRepeatParams& params)
{
    ReduceSumFirstStep<T>(sharedTmpBuffer, srcLocal, params);
    ReduceSumSecondStep<T>(dstLocal, sharedTmpBuffer, params);
    int32_t secondResultNum = DivCeil(params.repeatTimes, ONE_REPEAT_BYTE_SIZE / sizeof(T));
    ReduceSumFinalStep<T>(dstLocal, sharedTmpBuffer, secondResultNum);
}

template <typename T>
__aicore__ inline void ReduceImplSecondStepNoIndex(__ubuf__ T* sharedTmpBuffer, const ReduceMode& mode, int32_t& curData)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T); // fp16=128,fp32=64
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    uint64_t newMask[2];
    if (newRepeatTimes != 0) {
        CreateSpecialFormatMask<T>(elementNumPerRep / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        newMask[0] = lowMask;
        newMask[1] = highMask;
        struct ReduceRepeatParams newParams(newMask, newRepeatTimes, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
        ReduceOperation<T>(sharedTmpBuffer, sharedTmpBuffer, newParams, mode);
    }
    highMask = 0;
    lowMask = 0;
    if (leftData > 0) {
        CreateSpecialFormatMask<T>(leftData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        newMask[0] = lowMask;
        newMask[1] = highMask;
        struct ReduceRepeatParams leftParams(newMask, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
        ReduceOperation<T>(sharedTmpBuffer + newRepeatTimes * VREDUCE_PER_REP_OUTPUT,
            sharedTmpBuffer + newRepeatTimes * elementNumPerRep, leftParams, mode);
        newRepeatTimes += 1;
    }
    curData = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
}

template <typename T>
__aicore__ inline void ReduceImplThirdStepNoIndex(__ubuf__ T* dstLocal, __ubuf__ T* sharedTmpBuffer, const ReduceMode& mode,
    int32_t& curData)
{
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    uint64_t newMask[2];
    CreateSpecialFormatMask<T>(curData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
    newMask[0] = lowMask;
    newMask[1] = highMask;
    struct ReduceRepeatParams newParams(newMask, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);
    ReduceOperation<T>(sharedTmpBuffer, sharedTmpBuffer, newParams, mode);
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    *dstLocal = *sharedTmpBuffer;
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
__aicore__ inline void ReduceImplWithIndex(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, __ubuf__ T* sharedTmpBuffer,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    if (params.repeatTimes == 1) {
        ReduceOperation<T>(dstLocal, srcLocal, params, mode);
    } else {
        int32_t curData = 0;
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        ReduceImplFirstStep<T>(sharedTmpBuffer, srcLocal, params, mode, curData);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        int32_t secondStartPos = ((curData * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
        ReduceImplSecondStep<T>(sharedTmpBuffer, mode, curData, 0, secondStartPos);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        int32_t thirdStartPos =
            (((secondStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
        ReduceImplThirdStep<T>(dstLocal, sharedTmpBuffer, params.srcRepStride, mode, curData, secondStartPos, thirdStartPos);
    }
}

template <typename T>
__aicore__ inline void ReduceImplNoIndex(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, __ubuf__ T* sharedTmpBuffer,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    if (params.repeatTimes == 1) {
        ReduceOperation<T>(sharedTmpBuffer, srcLocal, params, mode);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *dstLocal = *sharedTmpBuffer;
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    } else {
        if (mode == ReduceMode::REDUCE_SUM) {
            ReduceSumImpl<T>(dstLocal, srcLocal, sharedTmpBuffer, params);
        } else {
            int32_t curData = 0;
            ReduceImplFirstStep<T>(sharedTmpBuffer, srcLocal, params, mode, curData);
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);

            ReduceImplSecondStepNoIndex<T>(sharedTmpBuffer, mode, curData);

            int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T); // fp16=128,fp32=64
            if (curData <= elementNumPerRep) {
                SetFlag<HardEvent::V_S>(eventIdVToS);
                WaitFlag<HardEvent::V_S>(eventIdVToS);
                ReduceImplThirdStepNoIndex<T>(dstLocal, sharedTmpBuffer, mode, curData);
                return;
            }
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);
            ReduceImplSecondStepNoIndex<T>(sharedTmpBuffer, mode, curData);
            if (curData <= elementNumPerRep) {
                SetFlag<HardEvent::V_S>(eventIdVToS);
                WaitFlag<HardEvent::V_S>(eventIdVToS);
                ReduceImplThirdStepNoIndex<T>(dstLocal, sharedTmpBuffer, mode, curData);
            }
        }
    }
}

template <typename T>
__aicore__ inline void ReduceImpl(__ubuf__ T* dstLocal, __ubuf__ T* srcLocal, __ubuf__ T* sharedTmpBuffer,
    struct ReduceRepeatParams& params, bool calIndex, const ReduceMode& mode)
{
    if (calIndex) {
        ReduceImplWithIndex<T>(dstLocal, srcLocal, sharedTmpBuffer, params, mode);
    } else {
        ReduceImplNoIndex<T>(dstLocal, srcLocal, sharedTmpBuffer, params, mode);
    }
}

template <typename T>
__aicore__ inline void ReduceTailCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count, bool calIndex, const ReduceMode& mode)
{
    using PrimType = PrimT<T>;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(PrimType); // fp16=128 , fp32=64
    int32_t repeatTime = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep; // tailCount  <= 128/64 repeatTime=1
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));

    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    PrimType bodyValue = dst.GetValue(0);
    PrimType bodyIndex = dst.GetValue(1);

    struct ReduceRepeatParams tailParams(tailCount, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);

    ReduceImpl<PrimType>((__ubuf__ PrimType*)dst.GetPhyAddr(), // dst
        (__ubuf__ PrimType*)src.GetPhyAddr(elementNumPerRep * repeatTime), (__ubuf__ PrimType*)sharedTmpBuffer.GetPhyAddr(),
        tailParams, calIndex, mode);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    PrimType tailValue = dst.GetValue(0);
    PrimType tailIndex = dst.GetValue(1);

    // bodyresult tailresult need vcmin/vcmax again
    struct ReduceRepeatParams lastParams(2, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    sharedTmpBuffer.SetValue(0, bodyValue);
    sharedTmpBuffer.SetValue(1, tailValue);
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);

    ReduceImpl<PrimType>((__ubuf__ PrimType*)dst.GetPhyAddr(), (__ubuf__ PrimType*)sharedTmpBuffer.GetPhyAddr(),
        (__ubuf__ PrimType*)sharedTmpBuffer.GetPhyAddr(), lastParams, calIndex, mode);
    if (calIndex) {
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        PrimType lastIndexVal = dst.GetValue(1);
        uint32_t newIndex = 0;
        uint32_t lastIndex = 0;
        if constexpr (sizeof(PrimType) == sizeof(half)) {
            lastIndex = *reinterpret_cast<uint16_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTime + *reinterpret_cast<uint16_t*>(&tailIndex);
        } else {
            lastIndex = *reinterpret_cast<uint32_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTime + *reinterpret_cast<uint32_t*>(&tailIndex);
        }
        if (lastIndex == 1) {
            dst.SetValue(1, *reinterpret_cast<PrimType*>(&newIndex));
        } else {
            dst.SetValue(1, bodyIndex);
        }
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    }
}

template <typename T>
__aicore__ inline void GetReduceMaxMinCountImpl(uint32_t &maxMinValue, uint32_t &maxMinIndex)
{
    ASCENDC_ASSERT((false), "GetReduceMaxMinCount is not supported on current device");
}

template <typename T>
__aicore__ inline void GetReduceMaxMinCountImpl(uint32_t &maxMinValue)
{
    ASCENDC_ASSERT((false), "GetReduceMaxMinCount is not supported on current device");
}

template <typename T>
__aicore__ inline void GetReduceMaxMinCountImpl(T &maxMinValue, T &maxMinIndex)
{
    ASCENDC_ASSERT((false), "GetReduceMaxMinCount is not supported on current device");
}

template <typename T>
__aicore__ inline void GetReduceMaxMinCountImpl(T &maxMinValue)
{
    ASCENDC_ASSERT((false), "GetReduceMaxMinCount is not supported on current device");
}

template <typename T>
__aicore__ inline T GetAccValImpl()
{
    ASCENDC_ASSERT((false), "GetAccVal is not supported on current device");
    return 0;
}
} // namespace AscendC
#endif // ASCENDC_MODULE_OPERATOR_VEC_REDUCE_IMPL_H
