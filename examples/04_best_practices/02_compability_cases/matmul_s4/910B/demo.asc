/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

#include "acl/acl.h"
#include "kernel_operator.h"
#include "data_utils.h"
#include "lib/matrix/matmul/matmul.h"

template <class A_TYPE, class B_TYPE, class C_TYPE, class BIAS_TYPE>
__aicore__ inline void CalcGMOffset(int blockIdx, int usedCoreNum, TCubeTiling& param, int& offsetA, int& offsetB,
                                    int& offsetC, int& offsetBias, int32_t isTransposeAIn, int32_t isTransposeBIn)
{
    auto temp0 = AscendC::Ceil(param.M, param.singleCoreM);
    auto temp1 = AscendC::Ceil(param.N, param.singleCoreN);
    auto temp2 = AscendC::Ceil(param.Ka, param.singleCoreK);

    auto divideKcoreNum = usedCoreNum / temp2;

    auto mCoreIndx = (blockIdx % divideKcoreNum) % temp0;
    auto nCoreIndx = (blockIdx % divideKcoreNum) / temp0;
    auto subKindx = blockIdx / divideKcoreNum;

    offsetA = mCoreIndx * param.Ka * param.singleCoreM + subKindx * param.singleCoreK;

    offsetB = subKindx * param.singleCoreK * param.N + nCoreIndx * param.singleCoreN;

    offsetC = mCoreIndx * param.N * param.singleCoreM + nCoreIndx * param.singleCoreN;

    offsetBias = nCoreIndx * param.singleCoreN;

    // 尾块M
    int gmUseM = param.M - mCoreIndx * param.singleCoreM;
    param.singleCoreM = gmUseM < param.singleCoreM ? gmUseM : param.singleCoreM;

    // 尾块N
    int gmUseN = param.N - nCoreIndx * param.singleCoreN;
    param.singleCoreN = gmUseN < param.singleCoreN ? gmUseN : param.singleCoreN;

    // 尾块K
    int gmUseK = param.Ka - subKindx * param.singleCoreK;
    param.singleCoreK = gmUseK < param.singleCoreK ? gmUseK : param.singleCoreK;
}

template <typename T, typename U>
__aicore__ inline void CopyTiling(T* tiling, __gm__ U* tilingGM, AscendC::TPipe* tpipe)
{
    uint32_t* ptr = reinterpret_cast<uint32_t*>(tiling);
    auto tiling32 = reinterpret_cast<__gm__ uint32_t*>(tilingGM);

    for (int i = 0; i < sizeof(T) / sizeof(uint32_t); i++, ptr++) { *ptr = *(tiling32 + i); }
}

template <class A_TYPE, class B_TYPE, class C_TYPE, class BIAS_TYPE>
__aicore__ inline void MatMulKernel(GM_ADDR aGM, GM_ADDR bGM, GM_ADDR cGM, GM_ADDR biasGM, GM_ADDR tilingGM,
                                    GM_ADDR quantGM, GM_ADDR workspaceGM, int32_t isTransposeAIn,
                                    int32_t isTransposeBIn, int32_t quantMode)
{
    if ASCEND_IS_AIV {
        return;
    }
    AscendC::SetSysWorkspace(workspaceGM);
    if (GetSysWorkSpacePtr() == nullptr) {
        return;
    }
    using A_T = typename A_TYPE::T;
    using B_T = typename B_TYPE::T;
    using C_T = typename C_TYPE::T;
    using BiasT = typename BIAS_TYPE::T;
    AscendC::TPipe que;
    TCubeTiling tiling;
    CopyTiling(&tiling, tilingGM, &que);
    if (block_idx >= tiling.usedCoreNum) {
        return;
    }
    bool isTransposeA = isTransposeAIn > 0 ? true : false;
    bool isTransposeB = isTransposeBIn > 0 ? true : false;
    AscendC::GlobalTensor<A_T> aGlobal;
    AscendC::GlobalTensor<B_T> bGlobal;
    AscendC::GlobalTensor<C_T> cGlobal;
    AscendC::GlobalTensor<BiasT> biasGlobal;
    AscendC::GlobalTensor<uint64_t> quantGlobal;
    quantGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ uint64_t*>(quantGM), tiling.N);
    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ A_T*>(aGM), tiling.M * tiling.Ka);
    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ B_T*>(bGM), tiling.Kb * tiling.N);
    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ C_T*>(cGM), tiling.M * tiling.N);
    biasGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ BiasT*>(biasGM), tiling.N);
    int offsetA = 0;
    int offsetB = 0;
    int offsetC = 0;
    int offsetBias = 0;
    CalcGMOffset<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE>(block_idx, tiling.usedCoreNum, tiling, offsetA, offsetB, offsetC,
                                                    offsetBias, isTransposeAIn, isTransposeBIn);

    auto gmA = aGlobal[offsetA];
    auto gmB = bGlobal[offsetB];
    auto gmC = cGlobal[offsetC];
    auto gmBias = biasGlobal[offsetBias];
    auto gmQuant = quantGlobal[offsetBias];
    AscendC::MatmulImpl<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE, CFG_MDL> mm;
    mm.SetSubBlockIdx(0);
    mm.Init(&tiling, &que);
    mm.SetTensorA(gmA, isTransposeA);
    mm.SetTensorB(gmB, isTransposeB);
    mm.IterateAll(gmC);
}

__global__ __aicore__ void demo(GM_ADDR aGM, GM_ADDR bGM, GM_ADDR cGM, GM_ADDR biasGM, GM_ADDR tilingGM,
                                GM_ADDR quantGM, GM_ADDR workspaceGM)
{
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, AscendC::int4b_t, 0> aType;
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, AscendC::int4b_t, 0> bType;
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, int32_t> cType;
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, int32_t> biasType;
    MatMulKernel<aType, bType, cType, biasType>(aGM, bGM, cGM, biasGM, tilingGM, quantGM, workspaceGM, 0, 0, 0);
}

int32_t main(int32_t argc, char* argv[])
{
    uint32_t blockDim = 1;
    aclInit(nullptr);
    int32_t deviceId = 0;
    aclrtSetDevice(deviceId);
    aclrtStream stream = nullptr;
    aclrtCreateStream(&stream);

    // 计算host侧所需空间大小
    int32_t m = 16, n = 64, k = 32;
    size_t aInputByteSize = static_cast<size_t>(1) * m * k * sizeof(AscendC::int4b_t);
    size_t bInputByteSize = static_cast<size_t>(1) * k * n * sizeof(AscendC::int4b_t);
    size_t outputByteSize = static_cast<size_t>(1) * m * n * sizeof(int32_t);
    size_t biasInputByteSize = static_cast<size_t>(1) * 64 * sizeof(int32_t);
    size_t tilingInputByteSize = static_cast<size_t>(1) * 64 * sizeof(int32_t);
    size_t quantInputByteSize = static_cast<size_t>(1) * 64 * sizeof(uint64_t);
    size_t workspaceInputByteSize = static_cast<size_t>(1) * 16777 * sizeof(float);
    // 初始化host和device侧指针， 一一对应
    uint8_t *xHost, *yHost, *zHost, *biasHost, *tilingHost, *quantHost, *workspaceHost;
    uint8_t *xDevice, *yDevice, *zDevice, *biasDevice, *tilingDevice, *quantDevice, *workspaceDevice;
    // 分配Host内存
    aclrtMallocHost((void**)(&xHost), aInputByteSize);
    aclrtMallocHost((void**)(&yHost), bInputByteSize);
    aclrtMallocHost((void**)(&zHost), outputByteSize);
    aclrtMallocHost((void**)(&biasHost), biasInputByteSize);
    aclrtMallocHost((void**)(&tilingHost), tilingInputByteSize);
    aclrtMallocHost((void**)(&quantHost), quantInputByteSize);
    aclrtMallocHost((void**)(&workspaceHost), workspaceInputByteSize);
    // 分配device内存
    aclrtMalloc((void**)&xDevice, aInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&yDevice, bInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&zDevice, outputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&biasDevice, biasInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&tilingDevice, tilingInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&quantDevice, quantInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&workspaceDevice, workspaceInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    // readfile
    ReadFile("./input/input_x.bin", aInputByteSize, xHost, aInputByteSize);
    ReadFile("./input/input_y.bin", bInputByteSize, yHost, bInputByteSize);
    ReadFile("./input/input_bias.bin", biasInputByteSize, biasHost, biasInputByteSize);
    ReadFile("./input/input_tiling.bin", tilingInputByteSize, tilingHost, tilingInputByteSize);
    ReadFile("./input/input_quant.bin", quantInputByteSize, quantHost, quantInputByteSize);
    ReadFile("./input/input_workspace.bin", workspaceInputByteSize, workspaceHost, workspaceInputByteSize);
    // host to device
    aclrtMemcpy(xDevice, aInputByteSize, xHost, aInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(yDevice, bInputByteSize, yHost, bInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(biasDevice, biasInputByteSize, biasHost, biasInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(tilingDevice, tilingInputByteSize, tilingHost, tilingInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(quantDevice, quantInputByteSize, quantHost, quantInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(workspaceDevice, workspaceInputByteSize, workspaceHost, workspaceInputByteSize,
                ACL_MEMCPY_HOST_TO_DEVICE);
    // call kernel
    demo<<<blockDim, nullptr, stream>>>(xDevice, yDevice, zDevice, biasDevice, tilingDevice, quantDevice,
                                        workspaceDevice);
    aclrtSynchronizeStream(stream);
    aclrtMemcpy(zHost, outputByteSize, zDevice, outputByteSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output.bin", zHost, outputByteSize);
    // free device
    aclrtFree(xDevice);
    aclrtFree(yDevice);
    aclrtFree(biasDevice);
    aclrtFree(tilingDevice);
    aclrtFree(quantDevice);
    aclrtFree(workspaceDevice);
    aclrtFree(zDevice);
    // free host
    aclrtFreeHost(xHost);
    aclrtFreeHost(yHost);
    aclrtFreeHost(biasHost);
    aclrtFreeHost(tilingHost);
    aclrtFreeHost(quantHost);
    aclrtFreeHost(workspaceHost);
    aclrtFreeHost(zHost);

    aclrtDestroyStream(stream);
    aclrtResetDevice(deviceId);
    aclFinalize();

    return 0;
}