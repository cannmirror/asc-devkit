/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/


/*!
 * \file mata_address_conflict.asc
 * \brief
 */
#include <cstdint>
#include "data_utils.h"
#include "kernel_operator.h"
#include "acl/acl.h"

constexpr uint32_t TILINGDATA_SIZE = 5;
constexpr uint32_t TILING_KEY_IS = 3;
constexpr uint32_t USED_CORE_NUM = 16;
struct AddsCustomTilingData 
{
    uint32_t m;
    uint32_t n;
    uint32_t tileM;
    uint32_t tileN;
    uint32_t loopOneCore;
};


bool GenerateTiling(uint8_t *tilingBuf)
{

    AddsCustomTilingData *tiling = reinterpret_cast<AddsCustomTilingData *>(tilingBuf);
    constexpr uint32_t M = 8192;
    constexpr uint32_t N = 128;
    constexpr uint32_t TILE_M = 512;
    constexpr uint32_t TILE_N = 8;
    constexpr uint32_t LOOP_ONE_CORE = M / TILE_M;
    tiling->m = M;
    tiling->n = N;
    tiling->tileM = TILE_M;
    tiling->tileN = TILE_N;
    tiling->loopOneCore = LOOP_ONE_CORE;
    return true;
}


using AscendC::TPosition;
class KernelAddsV1 {
public:
    __aicore__ inline KernelAddsV1() {}
    __aicore__ inline void Init(GM_ADDR x, GM_ADDR z, AddsCustomTilingData *tilingPtr, AscendC::TPipe* pipeIn)
    {
        tiling = tilingPtr;
        pipe = pipeIn;
        xGm.SetGlobalBuffer((__gm__ float *)x + AscendC::GetBlockIdx() * tiling->tileN);
        zGm.SetGlobalBuffer((__gm__ float *)z + AscendC::GetBlockIdx() * tiling->tileN);
        // the gm address conflict happens when multi cores visit the same addr range(512Bytes)
        // we disable the L2 cache mode to highlight the influence of the gm address conflict
        xGm.SetL2CacheHint(AscendC::CacheMode::CACHE_MODE_DISABLE);
        zGm.SetL2CacheHint(AscendC::CacheMode::CACHE_MODE_DISABLE);
        pipe->InitBuffer(inQueueX, BUFFER_NUM, tiling->tileM * tiling->tileN * sizeof(float));
        pipe->InitBuffer(outQueueZ, BUFFER_NUM, tiling->tileM * tiling->tileN * sizeof(float));
    }
    __aicore__ inline void Process()
    {
        for (int32_t i = 0; i < tiling->loopOneCore; i++) {
            // the following two SyncAll in this case are unnecessary actually,
            // we just used them to highlight the influence of gm address conflict in each loop
            AscendC::SyncAll();
            CopyIn(i);
            Compute();
            AscendC::SyncAll();
            CopyOut(i);
        }
    }

private:
    __aicore__ inline void CopyIn(int32_t progress)
    {
        AscendC::LocalTensor<float> xLocal = inQueueX.AllocTensor<float>();
        AscendC::DataCopyParams params;
        params.blockCount = tiling->tileM;
        params.blockLen = tiling->tileN * sizeof(float) / BLOCK_SIZE;
        params.srcStride = (tiling->n - tiling->tileN) * sizeof(float) / BLOCK_SIZE;
        params.dstStride = 0;
        AscendC::DataCopy(xLocal, xGm[progress * tiling->tileM * tiling->n], params);
        inQueueX.EnQue(xLocal);
    }
    __aicore__ inline void Compute()
    {
        AscendC::LocalTensor<float> xLocal = inQueueX.DeQue<float>();
        AscendC::LocalTensor<float> zLocal = outQueueZ.AllocTensor<float>();
        constexpr float scale = 2.0;
        AscendC::Adds(zLocal, xLocal, scale, tiling->tileM * tiling->tileN);
        outQueueZ.EnQue<float>(zLocal);
        inQueueX.FreeTensor(xLocal);
    }
    __aicore__ inline void CopyOut(int32_t progress)
    {
        AscendC::LocalTensor<float> zLocal = outQueueZ.DeQue<float>();
        AscendC::DataCopyParams params;
        params.blockCount = tiling->tileM;
        params.blockLen = tiling->tileN * sizeof(float) / BLOCK_SIZE;
        params.srcStride = 0;
        params.dstStride = (tiling->n - tiling->tileN) * sizeof(float) / BLOCK_SIZE;
        AscendC::DataCopy(zGm[progress * tiling->tileM * tiling->n], zLocal, params);
        outQueueZ.FreeTensor(zLocal);
    }

private:
    static constexpr int32_t BUFFER_NUM = 2;
    static constexpr int32_t BLOCK_SIZE = 32;

    AscendC::TPipe* pipe;
    AscendC::TQue<AscendC::TPosition::VECIN, BUFFER_NUM> inQueueX;
    AscendC::TQue<AscendC::TPosition::VECOUT, BUFFER_NUM> outQueueZ;
    AscendC::GlobalTensor<float> xGm;
    AscendC::GlobalTensor<float> zGm;
    AddsCustomTilingData *tiling;
};

class KernelAddsV2 {
public:
    __aicore__ inline KernelAddsV2() {}
    __aicore__ inline void Init(GM_ADDR x, GM_ADDR z, AddsCustomTilingData *tilingPtr, AscendC::TPipe* pipeIn)
    {
        tiling = tilingPtr;
        pipe = pipeIn;
        xGm.SetGlobalBuffer((__gm__ float *)x + AscendC::GetBlockIdx() * tiling->tileN);
        zGm.SetGlobalBuffer((__gm__ float *)z + AscendC::GetBlockIdx() * tiling->tileN);
        // the gm address conflict happens when multi cores visit the same addr range(512Bytes)
        // we disable the L2 cache mode to highlight the influence of the gm address conflict
        xGm.SetL2CacheHint(AscendC::CacheMode::CACHE_MODE_DISABLE);
        zGm.SetL2CacheHint(AscendC::CacheMode::CACHE_MODE_DISABLE);
        pipe->InitBuffer(inQueueX, BUFFER_NUM, tiling->tileM * tiling->tileN * sizeof(float));
        pipe->InitBuffer(outQueueZ, BUFFER_NUM, tiling->tileM * tiling->tileN * sizeof(float));
    }
    __aicore__ inline void Process()
    {
        for (int32_t i = 0; i < tiling->loopOneCore; i++) {
            // adjust the loop order to avoid the gm address conflict:
            // the loop order of core0  : 0, 1, 2, 3, ..., 13, 14, 15
            // the loop order of core1  : 1, 2, 3, 4, ..., 14, 15, 0
            // ...
            // the loop order of core15 : 15, 0, 1, 2, ..., 12, 13, 14
            int32_t newProgress = (i + AscendC::GetBlockIdx()) % tiling->loopOneCore;
            // the following two SyncAll in this case are unnecessary actually,
            // we just used them to highlight the influence of gm address conflict in each loop
            AscendC::SyncAll();
            CopyIn(newProgress);
            Compute();
            AscendC::SyncAll();
            CopyOut(newProgress);
        }
    }

private:
    __aicore__ inline void CopyIn(int32_t progress)
    {
        AscendC::LocalTensor<float> xLocal = inQueueX.AllocTensor<float>();
        AscendC::DataCopyParams params;
        params.blockCount = tiling->tileM;
        params.blockLen = tiling->tileN * sizeof(float) / BLOCK_SIZE;
        params.srcStride = (tiling->n - tiling->tileN) * sizeof(float) / BLOCK_SIZE;
        params.dstStride = 0;
        AscendC::DataCopy(xLocal, xGm[progress * tiling->tileM * tiling->n], params);
        inQueueX.EnQue(xLocal);
    }
    __aicore__ inline void Compute()
    {
        AscendC::LocalTensor<float> xLocal = inQueueX.DeQue<float>();
        AscendC::LocalTensor<float> zLocal = outQueueZ.AllocTensor<float>();
        constexpr float scale = 2.0;
        AscendC::Adds(zLocal, xLocal, scale, tiling->tileM * tiling->tileN);
        outQueueZ.EnQue<float>(zLocal);
        inQueueX.FreeTensor(xLocal);
    }
    __aicore__ inline void CopyOut(int32_t progress)
    {
        AscendC::LocalTensor<float> zLocal = outQueueZ.DeQue<float>();
        AscendC::DataCopyParams params;
        params.blockCount = tiling->tileM;
        params.blockLen = tiling->tileN * sizeof(float) / BLOCK_SIZE;
        params.srcStride = 0;
        params.dstStride = (tiling->n - tiling->tileN) * sizeof(float) / BLOCK_SIZE;
        AscendC::DataCopy(zGm[progress * tiling->tileM * tiling->n], zLocal, params);
        outQueueZ.FreeTensor(zLocal);
    }

private:
    static constexpr int32_t BUFFER_NUM = 2;
    static constexpr int32_t BLOCK_SIZE = 32;

    AscendC::TPipe* pipe;
    AscendC::TQue<AscendC::TPosition::VECIN, BUFFER_NUM> inQueueX;
    AscendC::TQue<AscendC::TPosition::VECOUT, BUFFER_NUM> outQueueZ;
    AscendC::GlobalTensor<float> xGm;
    AscendC::GlobalTensor<float> zGm;
    AddsCustomTilingData *tiling;
};

class KernelAddsV3 {
public:
    __aicore__ inline KernelAddsV3() {}
    __aicore__ inline void Init(GM_ADDR x, GM_ADDR z, AddsCustomTilingData *tilingPtr, AscendC::TPipe* pipeIn)
    {
        tiling = tilingPtr;
        pipe = pipeIn;
        // change the tile method from column split to row split
        xGm.SetGlobalBuffer((__gm__ float *)x + AscendC::GetBlockIdx() * tiling->tileM * tiling->n);
        zGm.SetGlobalBuffer((__gm__ float *)z + AscendC::GetBlockIdx() * tiling->tileM * tiling->n);
        // the gm address conflict happens when multi cores visit the same addr range(512Bytes)
        // we disable the L2 cache mode to highlight the influence of the gm address conflict
        xGm.SetL2CacheHint(AscendC::CacheMode::CACHE_MODE_DISABLE);
        zGm.SetL2CacheHint(AscendC::CacheMode::CACHE_MODE_DISABLE);
        pipe->InitBuffer(inQueueX, BUFFER_NUM, tiling->tileM * tiling->tileN * sizeof(float));
        pipe->InitBuffer(outQueueZ, BUFFER_NUM, tiling->tileM * tiling->tileN * sizeof(float));
    }
    __aicore__ inline void Process()
    {
        for (int32_t i = 0; i < tiling->loopOneCore; i++) {
            // the following two SyncAll in this case are unnecessary actually,
            // we just used them to highlight the influence of gm address conflict in each loop
            AscendC::SyncAll();
            CopyIn(i);
            Compute();
            AscendC::SyncAll();
            CopyOut(i);
        }
    }

private:
    __aicore__ inline void CopyIn(int32_t progress)
    {
        AscendC::LocalTensor<float> xLocal = inQueueX.AllocTensor<float>();
        AscendC::DataCopyParams params;
        params.blockCount = tiling->tileM;
        params.blockLen = tiling->tileN * sizeof(float) / BLOCK_SIZE;
        params.srcStride = (tiling->n - tiling->tileN) * sizeof(float) / BLOCK_SIZE;
        params.dstStride = 0;
        AscendC::DataCopy(xLocal, xGm[progress * tiling->tileN], params);
        inQueueX.EnQue(xLocal);
    }
    __aicore__ inline void Compute()
    {
        AscendC::LocalTensor<float> xLocal = inQueueX.DeQue<float>();
        AscendC::LocalTensor<float> zLocal = outQueueZ.AllocTensor<float>();
        constexpr float scale = 2.0;
        AscendC::Adds(zLocal, xLocal, scale, tiling->tileM * tiling->tileN);
        outQueueZ.EnQue<float>(zLocal);
        inQueueX.FreeTensor(xLocal);
    }
    __aicore__ inline void CopyOut(int32_t progress)
    {
        AscendC::LocalTensor<float> zLocal = outQueueZ.DeQue<float>();
        AscendC::DataCopyParams params;
        params.blockCount = tiling->tileM;
        params.blockLen = tiling->tileN * sizeof(float) / BLOCK_SIZE;
        params.srcStride = 0;
        params.dstStride = (tiling->n - tiling->tileN) * sizeof(float) / BLOCK_SIZE;
        AscendC::DataCopy(zGm[progress * tiling->tileN], zLocal, params);
        outQueueZ.FreeTensor(zLocal);
    }

private:
    static constexpr int32_t BUFFER_NUM = 2;
    static constexpr int32_t BLOCK_SIZE = 32;

    AscendC::TPipe* pipe;
    AscendC::TQue<AscendC::TPosition::VECIN, BUFFER_NUM> inQueueX;
    AscendC::TQue<AscendC::TPosition::VECOUT, BUFFER_NUM> outQueueZ;
    AscendC::GlobalTensor<float> xGm;
    AscendC::GlobalTensor<float> zGm;
    AddsCustomTilingData *tiling;
};

extern "C" __global__ __aicore__ void adds_custom_v1(GM_ADDR x, GM_ADDR z, GM_ADDR workspace, AddsCustomTilingData tiling)
{
    AscendC::TPipe pipe;
    REGISTER_TILING_DEFAULT(AddsCustomTilingData);
    KERNEL_TASK_TYPE_DEFAULT(KERNEL_TYPE_MIX_AIV_1_0);
    KernelAddsV1 op;
    op.Init(x, z, &tiling, &pipe);
    op.Process();
}

extern "C" __global__ __aicore__ void adds_custom_v2(GM_ADDR x, GM_ADDR z, GM_ADDR workspace, AddsCustomTilingData tiling)
{
    AscendC::TPipe pipe;
    REGISTER_TILING_DEFAULT(AddsCustomTilingData);
    KERNEL_TASK_TYPE_DEFAULT(KERNEL_TYPE_MIX_AIV_1_0);
    KernelAddsV2 op;
    op.Init(x, z, &tiling, &pipe);
    op.Process();
}

extern "C" __global__ __aicore__ void adds_custom_v3(GM_ADDR x, GM_ADDR z, GM_ADDR workspace, AddsCustomTilingData tiling)
{
    AscendC::TPipe pipe;
    REGISTER_TILING_DEFAULT(AddsCustomTilingData);
    KERNEL_TASK_TYPE_DEFAULT(KERNEL_TYPE_MIX_AIV_1_0);
    KernelAddsV3 op;
    op.Init(x, z, &tiling, &pipe);
    op.Process();
}

int32_t main(int32_t argc, char *argv[]) {
    uint8_t *tiling = nullptr;
    size_t tilingSize = TILINGDATA_SIZE * sizeof(uint32_t);

    aclInit(nullptr);
    int32_t deviceId = 0;
    aclrtSetDevice(deviceId);
    aclrtStream stream = nullptr;
    aclrtCreateStream(&stream);

    uint8_t *xHost, *yHost;
    uint8_t *xDevice, *yDevice;

    aclrtMallocHost((void **)(&tiling), tilingSize);

    GenerateTiling(tiling);

    auto tilingData = reinterpret_cast<AddsCustomTilingData *>(tiling);

    size_t inputSize = tilingData->m * tilingData->n * sizeof(float);
    size_t outputSize = tilingData->m * tilingData->n * sizeof(float);

    aclrtMallocHost((void **)(&xHost), inputSize);
    aclrtMallocHost((void **)(&yHost), outputSize);

    aclrtMalloc((void **)&xDevice, inputSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void **)&yDevice, outputSize, ACL_MEM_MALLOC_HUGE_FIRST);

    ReadFile("./input/input_x.bin", inputSize, xHost, inputSize);

    // Copy host memory to device memory
    aclrtMemcpy(xDevice, inputSize, xHost, inputSize, ACL_MEMCPY_HOST_TO_DEVICE);

    // Execute the kernel
    AddsCustomTilingData tiling_data;
    aclrtMemcpy(&tiling_data, sizeof(AddsCustomTilingData), tiling, sizeof(AddsCustomTilingData), ACL_MEMCPY_DEVICE_TO_HOST);

    adds_custom_v1<<<USED_CORE_NUM, nullptr, stream>>>(xDevice, yDevice, 0, tiling_data);
    // Wait for the stop event to complete
    aclrtSynchronizeStream(stream);
    // Copy result to host memory and write to output file
    aclrtMemcpy(yHost, outputSize, yDevice, outputSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output_z_1.bin", yHost, outputSize);

    adds_custom_v2<<<USED_CORE_NUM, nullptr, stream>>>(xDevice, yDevice, 0, tiling_data);
    // Wait for the stop event to complete
    aclrtSynchronizeStream(stream);
    // Copy result to host memory and write to output file
    aclrtMemcpy(yHost, outputSize, yDevice, outputSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output_z_2.bin", yHost, outputSize);

    adds_custom_v3<<<USED_CORE_NUM, nullptr, stream>>>(xDevice, yDevice, 0, tiling_data);
    // Wait for the stop event to complete
    aclrtSynchronizeStream(stream);
    // Copy result to host memory and write to output file
    aclrtMemcpy(yHost, outputSize, yDevice, outputSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output_z_3.bin", yHost, outputSize);

    // Clean up memory
    aclrtFree(xDevice);
    aclrtFree(yDevice);
    aclrtFreeHost(xHost);
    aclrtFreeHost(yHost);
    aclrtFreeHost(tiling);

    aclrtDestroyStream(stream);
    aclrtResetDevice(deviceId);
    aclFinalize();

    return 0;
}
