/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/


#include "acl/acl.h"
#include "kernel_operator.h"
#include "data_utils.h"
#include "lib/matrix/matmul/matmul.h"

template <class A_TYPE, class B_TYPE, class C_TYPE, class BIAS_TYPE>
__aicore__ inline void CalcGMOffset(int blockIdx, int usedCoreNum, TCubeTiling& param, int& offsetA, int& offsetB,
                                    int& offsetC, int& offsetBias, int32_t isTransposeAIn, int32_t isTransposeBIn)
{
    auto temp0 = AscendC::Ceil(param.M, param.singleCoreM);
    auto temp1 = AscendC::Ceil(param.N, param.singleCoreN);
    auto temp2 = AscendC::Ceil(param.Ka, param.singleCoreK);

    auto divideKcoreNum = usedCoreNum / temp2;

    auto mCoreIndx = (blockIdx % divideKcoreNum) % temp0;
    auto nCoreIndx = (blockIdx % divideKcoreNum) / temp0;
    auto subKindx = blockIdx / divideKcoreNum;

    offsetA = mCoreIndx * param.Ka * param.singleCoreM + subKindx * param.singleCoreK;
    offsetB = subKindx * param.singleCoreK * param.N + nCoreIndx * param.singleCoreN;
    offsetC = mCoreIndx * param.N * param.singleCoreM + nCoreIndx * param.singleCoreN;
    offsetBias = nCoreIndx * param.singleCoreN;

    // 尾块M
    int gmUseM = param.M - mCoreIndx * param.singleCoreM;
    param.singleCoreM = gmUseM < param.singleCoreM ? gmUseM : param.singleCoreM;

    // 尾块N
    int gmUseN = param.N - nCoreIndx * param.singleCoreN;
    param.singleCoreN = gmUseN < param.singleCoreN ? gmUseN : param.singleCoreN;

    // 尾块K
    int gmUseK = param.Ka - subKindx * param.singleCoreK;
    param.singleCoreK = gmUseK < param.singleCoreK ? gmUseK : param.singleCoreK;
}

template <typename T, typename U>
__aicore__ inline void CopyTiling(T* tiling, __gm__ U* tilingGM, AscendC::TPipe* tpipe)
{
    uint32_t* ptr = reinterpret_cast<uint32_t*>(tiling);
    auto tiling32 = reinterpret_cast<__gm__ uint32_t*>(tilingGM);

    for (int i = 0; i < sizeof(T) / sizeof(uint32_t); i++, ptr++) { *ptr = *(tiling32 + i); }
    return;
}

template <class A_TYPE, class B_TYPE, class C_TYPE, class BIAS_TYPE>
__aicore__ inline void MatMulKernel(AscendC::GlobalTensor<int8_t>& aGlobal, AscendC::GlobalTensor<int8_t>& bGlobal,
                                    AscendC::GlobalTensor<int32_t>& cGlobal, GM_ADDR tilingGM, GM_ADDR workspaceGM,
                                    int32_t isTransposeAIn, int32_t isTransposeBIn, AscendC::TPipe& que)
{
    using A_T = typename A_TYPE::T;
    using B_T = typename B_TYPE::T;
    using C_T = typename C_TYPE::T;
    using BiasT = typename BIAS_TYPE::T;
    AscendC::SetSysWorkspace(workspaceGM);
    if (GetSysWorkSpacePtr() == nullptr) {
        return;
    }
    TCubeTiling tiling;
    CopyTiling(&tiling, tilingGM, &que);
    bool isTransposeA = isTransposeAIn > 0 ? true : false;
    bool isTransposeB = isTransposeBIn > 0 ? true : false;
    if (block_idx >= tiling.usedCoreNum) {
        return;
    }
    int offsetA = 0;
    int offsetB = 0;
    int offsetC = 0;
    int offsetBias = 0;
    CalcGMOffset<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE>(block_idx, tiling.usedCoreNum, tiling, offsetA, offsetB, offsetC,
                                                    offsetBias, isTransposeAIn, isTransposeBIn);
    auto gmA = aGlobal[offsetA];
    auto gmB = bGlobal[offsetB];
    auto gmC = cGlobal[offsetC];
    AscendC::MatmulImpl<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE, CFG_MDL> mm;
    mm.SetSubBlockIdx(0);
    mm.Init(&tiling, &que);
    AscendC::LocalTensor<A_T> bufferLeft;
    AscendC::LocalTensor<B_T> bufferRight;
    AscendC::LocalTensor<C_T> bufferC;
    mm.SetTensorA(gmA, isTransposeA);
    mm.SetTensorB(gmB, isTransposeB);
    mm.IterateAll(gmC);
}

__aicore__ inline void Unzip(AscendC::GlobalTensor<int8_t>& dstGlobalTensor,
                             AscendC::GlobalTensor<int8_t>& srcGlobalTensor, uint32_t count,
                             AscendC::TQue<AscendC::TPosition::VECIN, 1>& q1,
                             AscendC::TQue<AscendC::TPosition::VECOUT, 1>& q2,
                             AscendC::TQue<AscendC::TPosition::VECOUT, 1>& q3)
{
    AscendC::LocalTensor<int8_t> srcLocalTensor = q1.AllocTensor<int8_t>();
    AscendC::LocalTensor<half> tmpTensor = q2.AllocTensor<half>();
    AscendC::LocalTensor<int8_t> dstLocalTensor = q3.AllocTensor<int8_t>();
    AscendC::DataCopy(srcLocalTensor, srcGlobalTensor, count);
    AscendC::LocalTensor<AscendC::int4b_t> int4SrcLocalTensor = srcLocalTensor.ReinterpretCast<AscendC::int4b_t>();
    uint32_t mask = count / sizeof(half);
    AscendC::SetFlag<AscendC::HardEvent::MTE2_V>(EVENT_ID0);
    AscendC::WaitFlag<AscendC::HardEvent::MTE2_V>(EVENT_ID0);
    AscendC::Cast<half, AscendC::int4b_t>(tmpTensor, int4SrcLocalTensor, AscendC::RoundMode::CAST_NONE, count * 2);
    AscendC::Cast<int8_t, half>(dstLocalTensor, tmpTensor, AscendC::RoundMode::CAST_CEIL, count * 2);
    AscendC::SetFlag<AscendC::HardEvent::V_MTE3>(EVENT_ID1);
    AscendC::WaitFlag<AscendC::HardEvent::V_MTE3>(EVENT_ID1);
    AscendC::DataCopy(dstGlobalTensor, dstLocalTensor, count * 2);
    q1.FreeTensor(srcLocalTensor);
    q2.FreeTensor(tmpTensor);
    q3.FreeTensor(dstLocalTensor);
}

__global__ __aicore__ void matmul_s4(GM_ADDR a4GM, GM_ADDR b4GM, GM_ADDR aGM, GM_ADDR bGM, GM_ADDR cGM, GM_ADDR tilingGM,
                                GM_ADDR workspaceGM)
{
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, int8_t, false> aType;
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, int8_t, false> bType;
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND_ALIGN, int32_t> cType;
    typedef AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND_ALIGN, int32_t> biasType;
    AscendC::TPipe pipe;
    TCubeTiling tiling;
    CopyTiling(&tiling, tilingGM, &pipe);
    AscendC::GlobalTensor<int8_t> aGlobal;
    AscendC::GlobalTensor<int8_t> a4Global;
    AscendC::GlobalTensor<int8_t> bGlobal;
    AscendC::GlobalTensor<int8_t> b4Global;
    AscendC::GlobalTensor<int32_t> cGlobal;
    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(aGM), tiling.M * tiling.Ka);
    a4Global.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(a4GM), tiling.M * tiling.Ka / 2);
    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(bGM), tiling.Kb * tiling.N);
    b4Global.SetGlobalBuffer(reinterpret_cast<__gm__ int8_t*>(b4GM), tiling.Kb * tiling.N / 2);
    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t*>(cGM), tiling.M * tiling.N);
    AscendC::TQue<AscendC::TPosition::VECIN, 1> q1;
    AscendC::TQue<AscendC::TPosition::VECOUT, 1> q2;
    AscendC::TQue<AscendC::TPosition::VECOUT, 1> q3;
    uint16_t x = 256;
    uint16_t y = 256;
    pipe.InitBuffer(q1, 1, sizeof(int8_t) * x * y);
    pipe.InitBuffer(q2, 1, sizeof(half) * x * y);
    pipe.InitBuffer(q3, 1, sizeof(int8_t) * x * y);
    if ASCEND_IS_AIV {
        Unzip(aGlobal, a4Global, tiling.M * tiling.Ka / 2, q1, q2, q3);
        Unzip(bGlobal, b4Global, tiling.Kb * tiling.N / 2, q1, q2, q3);
        AscendC::CrossCoreSetFlag<4, PIPE_MTE3>(8);
    }
    if ASCEND_IS_AIC {
        AscendC::CrossCoreWaitFlag<4, PIPE_MTE2>(8);
        MatMulKernel<aType, bType, cType, biasType>(aGlobal, bGlobal, cGlobal, tilingGM, workspaceGM, false, false,
                                                    pipe);
    }
}

int32_t main(int32_t argc, char* argv[])
{
    uint32_t blockDim = 1;
    aclInit(nullptr);
    int32_t deviceId = 0;
    aclrtSetDevice(deviceId);
    aclrtStream stream = nullptr;
    aclrtCreateStream(&stream);
    int32_t m = 256, n = 256, k = 256;
    size_t a4InputByteSize = static_cast<size_t>(1) * m * k * sizeof(int8_t) / 2;
    size_t b4InputByteSize = static_cast<size_t>(1) * k * n * sizeof(int8_t) / 2;
    size_t aInputByteSize = static_cast<size_t>(1) * m * k * sizeof(int8_t);
    size_t bInputByteSize = static_cast<size_t>(1) * k * n * sizeof(int8_t);
    size_t outputByteSize = static_cast<size_t>(1) * m * n * sizeof(float);
    size_t biasInputByteSize = static_cast<size_t>(1) * 256 * sizeof(int32_t);
    size_t tilingInputByteSize = static_cast<size_t>(1) * 64 * sizeof(int32_t);
    size_t quantInputByteSize = static_cast<size_t>(1) * 256 * sizeof(uint64_t);
    size_t workspaceInputByteSize = static_cast<size_t>(1) * 131072 * sizeof(float);
    uint8_t *xHost, *x4Host, *yHost, *y4Host, *zHost, *tilingHost, *workspaceHost;
    uint8_t *xDevice, *x4Device, *yDevice, *y4Device, *zDevice, *tilingDevice, *workspaceDevice;
    aclrtMallocHost((void**)(&xHost), aInputByteSize);
    aclrtMallocHost((void**)(&x4Host), a4InputByteSize);
    aclrtMallocHost((void**)(&yHost), bInputByteSize);
    aclrtMallocHost((void**)(&y4Host), b4InputByteSize);
    aclrtMallocHost((void**)(&zHost), outputByteSize);
    aclrtMallocHost((void**)(&tilingHost), tilingInputByteSize);
    aclrtMallocHost((void**)(&workspaceHost), workspaceInputByteSize);
    aclrtMalloc((void**)&xDevice, aInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&x4Device, a4InputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&yDevice, bInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&y4Device, b4InputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&zDevice, outputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&tilingDevice, tilingInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void**)&workspaceDevice, workspaceInputByteSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/input_x.bin", aInputByteSize, xHost, aInputByteSize);
    ReadFile("./input/input_x4.bin", a4InputByteSize, x4Host, a4InputByteSize);
    ReadFile("./input/input_y.bin", bInputByteSize, yHost, bInputByteSize);
    ReadFile("./input/input_y4.bin", b4InputByteSize, y4Host, b4InputByteSize);
    ReadFile("./input/input_tiling.bin", tilingInputByteSize, tilingHost, tilingInputByteSize);
    ReadFile("./input/input_workspace.bin", workspaceInputByteSize, workspaceHost, workspaceInputByteSize);
    aclrtMemcpy(xDevice, aInputByteSize, xHost, aInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(x4Device, a4InputByteSize, x4Host, a4InputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(yDevice, bInputByteSize, yHost, bInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(y4Device, b4InputByteSize, y4Host, b4InputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(tilingDevice, tilingInputByteSize, tilingHost, tilingInputByteSize, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(workspaceDevice, workspaceInputByteSize, workspaceHost, workspaceInputByteSize,
                ACL_MEMCPY_HOST_TO_DEVICE);
    matmul_s4<<<blockDim, nullptr, stream>>>(x4Device, y4Device, xDevice, yDevice, zDevice, tilingDevice, workspaceDevice);
    aclrtSynchronizeStream(stream);
    aclrtMemcpy(zHost, outputByteSize, zDevice, outputByteSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output.bin", zHost, outputByteSize);
    aclrtFree(xDevice);
    aclrtFree(x4Device);
    aclrtFree(yDevice);
    aclrtFree(y4Device);
    aclrtFree(tilingDevice);
    aclrtFree(workspaceDevice);
    aclrtFree(zDevice);
    aclrtFreeHost(xHost);
    aclrtFreeHost(x4Host);
    aclrtFreeHost(yHost);
    aclrtFreeHost(y4Host);
    aclrtFreeHost(tilingHost);
    aclrtFreeHost(workspaceHost);
    aclrtFreeHost(zHost);
    aclrtDestroyStream(stream);
    aclrtResetDevice(deviceId);
    aclFinalize();
    return 0;
}