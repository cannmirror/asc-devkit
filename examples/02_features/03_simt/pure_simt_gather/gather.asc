/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/


/* !
 * \file gather.asc
 * \brief
 */

#include <iostream>
#include <iterator>
#include <vector>
#include "acl/acl.h"
#include "tiling/platform/platform_ascendc.h"

namespace {
    constexpr uint32_t MAX_THREAD_COUNT = 2048;
    constexpr uint32_t MAX_BLOCK_COUNT = 65535;
}

template <typename type_data, typename type_idx>
__global__ __launch_bounds__(MAX_THREAD_COUNT) void gather_custom(
    type_data* input,
    type_idx* index,
    type_data* gather_output,
    uint32_t in_width,
    uint32_t index_total_length)
{
    // Calculate global thread ID
    int32_t out_row = blockIdx.x * blockDim.x + threadIdx.x;

    // Maps to the row index of output tensor
    if (out_row >= index_total_length) {
        return;
    }
    // Single thread processes entire row (all columns) - enables coalesced memory access
    uint32_t in_row = index[out_row];
    int input_idx = in_row * in_width;
    int output_idx = out_row * in_width;
    for (int32_t col = 0; col < in_width; col++) {
        gather_output[output_idx] = input[input_idx];
        input_idx += 1;
        output_idx += 1;
    }
}

bool block_splite(uint32_t index_total_length, uint32_t &numBlocks, uint32_t &thread_num_per_block) {
    uint32_t real_core_num = 0;
    const auto& platformInfoMgr = platform_ascendc::PlatformAscendCManager::GetInstance();
    if (platformInfoMgr == nullptr) {
        std::cout << "[ERROR] Get plateform info failed, please check device status."<< std::endl;
        return false;
    }
    real_core_num = platformInfoMgr->GetCoreNumAiv();
    numBlocks = real_core_num;
    thread_num_per_block= (index_total_length + numBlocks -1) / numBlocks;
    if (thread_num_per_block > MAX_THREAD_COUNT) {
        thread_num_per_block = MAX_THREAD_COUNT;
        numBlocks = (index_total_length + thread_num_per_block - 1) / thread_num_per_block;
        if (numBlocks > MAX_BLOCK_COUNT) {
        std::cout << "[ERROR] index_total_length: "<< index_total_length << " can not be bigger then "
            << MAX_THREAD_COUNT * MAX_BLOCK_COUNT<< "."<< std::endl;
        return false;
        }
    }
    return true;
}

std::vector<float> gather(std::vector<float>& input, const uint32_t* in_shape, std::vector<uint32_t>& index)
{
    std::vector<float> output_stub;
    uint32_t input_total_length = input.size();
    size_t input_total_byte_size = input_total_length * sizeof(float);

    uint32_t index_total_length = index.size();
    size_t index_total_byte_size = index_total_length * sizeof(uint32_t);

    uint32_t output_total_length = index.size() * in_shape[1];
    size_t output_total_byte_size = output_total_length * sizeof(float);
    // Calc splite params
    uint32_t numBlocks = 0;
    uint32_t thread_num_per_block = 0;
    if (!block_splite(index_total_length, numBlocks, thread_num_per_block)) {
        return {};
    }
    std::cout << "[INFO] Block number is "<< numBlocks << "." << std::endl;
    std::cout << "[INFO] Thread number in a block is "<< thread_num_per_block << "." << std::endl;

    int32_t device_id = 0;
    aclrtStream stream = nullptr;

    uint8_t* input_host = reinterpret_cast<uint8_t *>(input.data());
    uint8_t* index_host = reinterpret_cast<uint8_t *>(index.data());
    uint8_t* output_host = nullptr;
    float* input_device = nullptr;
    uint32_t* index_device = nullptr;
    float* output_device = nullptr;
    // Init
    aclInit(nullptr);
    aclrtSetDevice(device_id);
    aclrtCreateStream(&stream);
    // Malloc memory in host and device
    aclrtMallocHost((void **)(&output_host), output_total_byte_size);
    aclrtMalloc((void **)&input_device, input_total_byte_size, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void **)&index_device, index_total_byte_size, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMalloc((void **)&output_device, output_total_byte_size, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMemcpy(input_device, input_total_byte_size, input_host, input_total_byte_size, ACL_MEMCPY_HOST_TO_DEVICE);
    aclrtMemcpy(index_device, index_total_byte_size, index_host, index_total_byte_size, ACL_MEMCPY_HOST_TO_DEVICE);

    uint32_t dyn_ubuf_size = 0;  // No need to alloc dynamic memory.

    // Call kernel funtion with <<<...>>>
    gather_custom<<<numBlocks, thread_num_per_block, dyn_ubuf_size, stream>>>(
            input_device, index_device, output_device, in_shape[1], index_total_length);

    aclrtSynchronizeStream(stream);
    // Copy result from device to host
    aclrtMemcpy(output_host, output_total_byte_size, output_device, output_total_byte_size, ACL_MEMCPY_DEVICE_TO_HOST);
    std::vector<float> output((float *)output_host, (float *)(output_host + output_total_byte_size));
    // Free memory
    aclrtFree(input_device);
    aclrtFree(index_device);
    aclrtFree(output_device);
    aclrtFreeHost(output_host);
    // DeInt
    aclrtDestroyStream(stream);
    aclrtResetDevice(device_id);
    aclFinalize();
    return output;
}

uint32_t verify_result(std::vector<float>& output, std::vector<float>& golden)
{
    auto print_tensor = [](std::vector<float>& tensor, const char* name) {
        constexpr size_t max_print_size = 20;
        std::cout << name << ": ";
        std::copy(tensor.begin(), tensor.begin() + std::min(tensor.size(), max_print_size),
            std::ostream_iterator<float>(std::cout, " "));
        if (tensor.size() > max_print_size) {
            std::cout << "...";
        }
        std::cout << std::endl;
    };
    print_tensor(output, "Output");
    print_tensor(golden, "Golden");
    if (std::equal(output.begin(), output.end(), golden.begin())) {
        std::cout << "[Success] Case accuracy is verification passed." << std::endl;
        return 0;
    } else {
        std::cout << "[Failed] Case accuracy is verification failed!" << std::endl;
        return 1;
    }
    return 0;
}

int32_t main(int32_t argc, char* argv[])
{
    constexpr uint32_t in_shape[2] = {20001, 1023};
    constexpr uint32_t input_total_length = in_shape[0] * in_shape[1];
    std::vector<float> input(input_total_length);
    for (uint32_t i = 0; i < input_total_length; i++) {
        input[i] = i *1.2f;
    }

    constexpr uint32_t index_total_length = 12288;
    std::vector<uint32_t> index(index_total_length);
    for (uint32_t i = 0; i < index_total_length; i++) {
        if (i  % 2 == 0) {
            index[i] = i + 1;
        } else {
            index[i] = i + 2;
        }
        
    }

    std::vector<float> golden(index_total_length * in_shape[1]);
    for (uint32_t i = 0; i < index_total_length; i++) {
        for (uint32_t j = 0; j < in_shape[1]; j++) {
            golden[i * in_shape[1] + j] = input[index[i] * in_shape[1] + j];
        }
    }

    std::vector<float> output = gather(input, in_shape, index);
    if (output.empty()) {
        return 1;
    }
    return verify_result(output, golden);
}