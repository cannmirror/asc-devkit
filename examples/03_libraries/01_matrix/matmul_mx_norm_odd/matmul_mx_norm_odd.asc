/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/

/* !
 * \file matmul_mx_norm_odd.asc
 * \brief
 */

#include <iostream>
#include <sstream>
#include "data_utils.h"
#include "register/tilingdata_base.h"
#include "tiling/tiling_api.h"
#include "kernel_operator.h"
#include "acl/acl.h"
#include "tiling/platform/platform_ascendc.h"
#define ASCENDC_CUBE_ONLY
#include "lib/matmul_intf.h"

using namespace AscendC;

struct MatmulCaseParams
{
    int32_t usedCoreNum;
    int32_t m;
    int32_t n;
    int32_t k;
    bool isBias;
    bool isATrans;
    bool isBTrans;
};

TCubeTiling generateTiling(const MatmulCaseParams& testCaseParams)
{
    TCubeTiling tilingData;
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    matmul_tiling::MultiCoreMatmulTiling cubeTiling(*ascendcPlatform);
    
    uint32_t M = testCaseParams.m;
    uint32_t N = testCaseParams.n;
    uint32_t K = testCaseParams.k;
    uint32_t numBlocks = testCaseParams.usedCoreNum;
    bool isBias = testCaseParams.isBias;
    bool isAtrans = testCaseParams.isATrans;
    bool isBtrans = testCaseParams.isBTrans;

    cubeTiling.SetDim(1);
    cubeTiling.SetAType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT8_E5M2, isAtrans);
    cubeTiling.SetBType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT8_E5M2, isBtrans);
    cubeTiling.SetCType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT);
    cubeTiling.SetBiasType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT);
    
    cubeTiling.SetScaleAType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND, false);
    cubeTiling.SetScaleBType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND, false);

    cubeTiling.SetOrgShape(M, N, K);
    cubeTiling.SetShape(M, N, K);
    cubeTiling.EnableBias(isBias);
    cubeTiling.SetBufferSpace(-1, -1, -1);
    cubeTiling.SetMadType(matmul_tiling::MatrixMadType::MXMODE);

    if (cubeTiling.GetTiling(tilingData) == -1) {
        std::cout << "Generate tiling failed." << std::endl;
        return {};
    }
    
    return tilingData;
}

template <typename ATYPE, typename BType, typename CType, typename BiasType>
class MatmulKernel
{
public:
    __aicore__ inline MatmulKernel() {};
    
    __aicore__ inline void init(GM_ADDR a, GM_ADDR b, GM_ADDR as, GM_ADDR bs, GM_ADDR bias, GM_ADDR c, 
        const TCubeTiling& tiling, bool isTransA, bool isTransB);
    
    __aicore__ inline void process();

    using aType = AscendC::MatmulTypeWithScale<AscendC::TPosition::GM, AscendC::TPosition::GM, 
        CubeFormat::ND, fp8_e5m2_t, false>;
    using bType = AscendC::MatmulTypeWithScale<AscendC::TPosition::GM, AscendC::TPosition::GM, 
        CubeFormat::ND, fp8_e5m2_t, false>;
    using cType = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, float>;
    using biasType = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, float>;
    
    AscendC::Matmul<aType, bType, cType, biasType, CFG_MDL, 
        AscendC::MatmulCallBackFunc<nullptr, nullptr, nullptr>, 
        AscendC::Impl::Detail::MatmulWithScalePolicy> matmulObj;

private:
    __aicore__ inline void calcOffset(int32_t blockIdx, int32_t& offsetA, int32_t& offsetB, 
        int32_t& offsetAscale, int32_t& offsetBscale, int32_t& offsetC, int32_t& offsetBias);

    AscendC::GlobalTensor<fp8_e5m2_t> aGlobal;
    AscendC::GlobalTensor<fp8_e5m2_t> bGlobal;
    AscendC::GlobalTensor<fp8_e8m0_t> asGlobal;
    AscendC::GlobalTensor<fp8_e8m0_t> bsGlobal;
    AscendC::GlobalTensor<float> cGlobal;
    AscendC::GlobalTensor<float> biasGlobal;
    TCubeTiling tiling;
    int32_t mCoreIndex;
    int32_t nCoreIndex;
    bool isTransA{false};
    bool isTransB{false};
};

__aicore__ inline void copyTiling(TCubeTiling* tiling, GM_ADDR tilingGm)
{
    uint32_t* ptr = reinterpret_cast<uint32_t*>(tiling);
    auto tiling32 = reinterpret_cast<__gm__ uint32_t*>(tilingGm);

    for (int i = 0; i < sizeof(TCubeTiling) / sizeof(uint32_t); i++, ptr++) {
        *ptr = *(tiling32 + i);
    }
    
    return;
}

constexpr int SCALE_CEIL_NUMBER = 64;
constexpr int SCALE_NUMBER = 2;

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulKernel<AType, BType, CType, BiasType>::init(GM_ADDR a, GM_ADDR b, 
    GM_ADDR as, GM_ADDR bs, GM_ADDR bias, GM_ADDR c, const TCubeTiling& tiling, 
    bool isTransA, bool isTransB)
{
    this->tiling = tiling;
    this->isTransA = isTransA;
    this->isTransB = isTransB;
    
    int sK = (tiling.Ka + SCALE_CEIL_NUMBER - 1) / SCALE_CEIL_NUMBER * SCALE_NUMBER;
    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ fp8_e5m2_t*>(a), tiling.M * tiling.Ka);
    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ fp8_e5m2_t*>(b), tiling.Kb * tiling.N);
    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ CType*>(c), tiling.M * tiling.N);
    biasGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ BiasType*>(bias), tiling.N);
    asGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ fp8_e8m0_t*>(as), tiling.M * sK);
    bsGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ fp8_e8m0_t*>(bs), sK * tiling.N);

    int32_t offsetA = 0;
    int32_t offsetB = 0;
    int32_t offsetAscale = 0;
    int32_t offsetBscale = 0;
    int32_t offsetC = 0;
    int32_t offsetBias = 0;
    
    calcOffset(AscendC::GetBlockIdx(), offsetA, offsetB, offsetAscale, offsetBscale, offsetC, offsetBias);
    
    aGlobal = aGlobal[offsetA];
    bGlobal = bGlobal[offsetB];
    cGlobal = cGlobal[offsetC];
    asGlobal = asGlobal[offsetAscale];
    bsGlobal = bsGlobal[offsetBscale];
    biasGlobal = biasGlobal[offsetBias];
    
    if (GetSysWorkSpacePtr() == nullptr) {
        return;
    }
}

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulKernel<AType, BType, CType, BiasType>::process()
{
    if (AscendC::GetBlockIdx() >= tiling.usedCoreNum) {
        return;
    }

    int tailM = tiling.M - mCoreIndex * tiling.singleCoreM;
    tailM = tailM < tiling.singleCoreM ? tailM : tiling.singleCoreM;
    
    int tailN = tiling.N - nCoreIndex * tiling.singleCoreN;
    tailN = tailN < tiling.singleCoreN ? tailN : tiling.singleCoreN;
    
    if (tailM < tiling.singleCoreM || tailN < tiling.singleCoreN) {
        matmulObj.SetTail(tailM, tailN);
    }

    matmulObj.SetTensorA(aGlobal, isTransA);
    matmulObj.SetTensorB(bGlobal, isTransB);
    matmulObj.SetTensorScaleA(asGlobal, false);
    matmulObj.SetTensorScaleB(bsGlobal, false);
    
    if (tiling.isBias) {
        matmulObj.SetBias(biasGlobal);
    }

    matmulObj.IterateAll(cGlobal);
    matmulObj.End();
}

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulKernel<AType, BType, CType, BiasType>::calcOffset(
    int32_t blockIdx, int32_t& offsetA, int32_t& offsetB, int32_t& offsetAscale, 
    int32_t& offsetBscale, int32_t& offsetC, int32_t& offsetBias)
{
    const TCubeTiling& tiling = this->tiling;
    auto mSingleBlocks = (tiling.M + tiling.singleCoreM - 1) / tiling.singleCoreM;
    mCoreIndex = blockIdx % mSingleBlocks;
    nCoreIndex = blockIdx / mSingleBlocks;

    if (isTransA) {
        offsetA = mCoreIndex * tiling.singleCoreM;
    } else {
        offsetA = mCoreIndex * tiling.Ka * tiling.singleCoreM;
    }
    
    if (isTransB) {
        offsetB = nCoreIndex * tiling.Kb * tiling.singleCoreN;
    } else {
        offsetB = nCoreIndex * tiling.singleCoreN;
    }

    int sK = (tiling.Ka + SCALE_CEIL_NUMBER - 1) / SCALE_CEIL_NUMBER * SCALE_NUMBER;
    offsetAscale = mCoreIndex * sK * tiling.singleCoreM;
    offsetBscale = nCoreIndex * tiling.singleCoreN;
    
    offsetC = mCoreIndex * tiling.N * tiling.singleCoreM + nCoreIndex * tiling.singleCoreN;
    offsetBias = nCoreIndex * tiling.singleCoreN;
}

__cube__ __global__ void matmulMxNormOddCustom(
    GM_ADDR a, GM_ADDR b, GM_ADDR as, GM_ADDR bs, GM_ADDR bias, GM_ADDR c, 
    GM_ADDR workspace, GM_ADDR tilingGm)
{  
    TCubeTiling tiling;
    copyTiling(&tiling, tilingGm);
    
    MatmulKernel<fp8_e5m2_t, fp8_e5m2_t, float, float> matmulKernel;
    AscendC::TPipe pipe;
    
    REGIST_MATMUL_OBJ(&pipe, GetSysWorkSpacePtr(), matmulKernel.matmulObj, &tiling);
    
    matmulKernel.init(a, b, as, bs, bias, c, tiling, false, false);
    matmulKernel.process();
}

void matmulMxNormOddCustomDo(uint32_t numBlocks, void* stream,
    uint8_t* a, uint8_t* b, uint8_t* as, uint8_t* bs, uint8_t* bias, uint8_t* c, 
    uint8_t* workspace, uint8_t* tilingGm)
{
    matmulMxNormOddCustom<<<numBlocks, nullptr, stream>>>(a, b, as, bs, bias, c, workspace, tilingGm);
}

constexpr bool IS_BIAS = false;
constexpr bool IS_A_TRANS = false;
constexpr bool IS_B_TRANS = false;

struct MatrixFileSize
{
    size_t x1FileSize;
    size_t x2FileSize;
    size_t xs1FileSize;
    size_t xs2FileSize;
    size_t yFileSize;
    size_t biasFileSize;
};

static size_t getSysWorkSpaceSize()
{
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    
    if (ascendcPlatform == nullptr) {
        return 0;
    }
    
    return static_cast<size_t>(ascendcPlatform->GetLibApiWorkSpaceSize());
}

void matmulOp(uint8_t* x1, uint8_t* x2, uint8_t* xs1, uint8_t* xs2, uint8_t* y, uint8_t* bias, 
    int64_t m, int64_t n, int64_t k, void* stream = nullptr)
{
    uint8_t* workspaceDevice;
    size_t workspaceSize = getSysWorkSpaceSize();
    aclrtMalloc((void **)&workspaceDevice, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);

    uint8_t* tilingHost;
    uint8_t* tilingDevice;
    size_t tilingFileSize = sizeof(TCubeTiling);
    
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    MatmulCaseParams testCaseParams{static_cast<int32_t>(ascendcPlatform->GetCoreNumAic()),
        static_cast<int32_t>(m), static_cast<int32_t>(n), static_cast<int32_t>(k), 
        IS_BIAS, IS_A_TRANS, IS_B_TRANS};
    
    const auto tilingData = generateTiling(testCaseParams);
    
    aclrtMallocHost((void **)(&tilingHost), tilingFileSize);
    aclrtMalloc((void **)&tilingDevice, tilingFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    
    aclrtMemcpy(tilingHost, tilingFileSize, &tilingData, tilingFileSize, ACL_MEMCPY_HOST_TO_HOST);
    aclrtMemcpy(tilingDevice, tilingFileSize, tilingHost, tilingFileSize, ACL_MEMCPY_HOST_TO_DEVICE);
    
    matmulMxNormOddCustomDo(tilingData.usedCoreNum, stream, x1, x2, xs1, xs2, bias, y, 
        workspaceDevice, tilingDevice);
    
    aclrtFreeHost(tilingHost);
    aclrtFree(workspaceDevice);
    aclrtFree(tilingDevice);
}

void testAclInit(aclrtContext& context, aclrtStream& stream, int64_t& deviceId)
{
    aclInit(nullptr);
    aclrtSetDevice(deviceId);
    aclrtCreateContext(&context, deviceId);
    aclrtCreateStream(&stream);
}

void testAclDeInit(aclrtContext& context, aclrtStream& stream, int64_t& deviceId)
{
    aclrtDestroyStream(stream);
    aclrtDestroyContext(context);
    aclrtResetDevice(deviceId);
    aclFinalize();
}

void testMatmul(int64_t m, int64_t n, int64_t k, const MatrixFileSize& matrixFileSize)
{
    size_t x1FileSize = matrixFileSize.x1FileSize;
    size_t x2FileSize = matrixFileSize.x2FileSize;
    size_t xs1FileSize = matrixFileSize.xs1FileSize;
    size_t xs2FileSize = matrixFileSize.xs2FileSize;
    size_t yFileSize = matrixFileSize.yFileSize;
    size_t biasFileSize = matrixFileSize.biasFileSize;

    aclrtContext context;
    aclrtStream stream = nullptr;
    int64_t deviceId = 0;
    
    testAclInit(context, stream, deviceId);

    uint8_t* x1Host;
    uint8_t* x1Device;
    aclrtMallocHost((void **)(&x1Host), x1FileSize);
    aclrtMalloc((void **)&x1Device, x1FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x1_gm.bin", x1FileSize, x1Host, x1FileSize);
    aclrtMemcpy(x1Device, x1FileSize, x1Host, x1FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* x2Host;
    uint8_t* x2Device;
    aclrtMallocHost((void **)(&x2Host), x2FileSize);
    aclrtMalloc((void **)&x2Device, x2FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x2_gm.bin", x2FileSize, x2Host, x2FileSize);
    aclrtMemcpy(x2Device, x2FileSize, x2Host, x2FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* xs1Host;
    uint8_t* xs1Device;
    aclrtMallocHost((void **)(&xs1Host), xs1FileSize);
    aclrtMalloc((void **)&xs1Device, xs1FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x1_mx_gm.bin", xs1FileSize, xs1Host, xs1FileSize);
    aclrtMemcpy(xs1Device, xs1FileSize, xs1Host, xs1FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* xs2Host;
    uint8_t* xs2Device;
    aclrtMallocHost((void **)(&xs2Host), xs2FileSize);
    aclrtMalloc((void **)&xs2Device, xs2FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x2_mx_gm.bin", xs2FileSize, xs2Host, xs2FileSize);
    aclrtMemcpy(xs2Device, xs2FileSize, xs2Host, xs2FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* biasHost = nullptr;
    uint8_t* biasDevice = nullptr;
    
    if (IS_BIAS) {
        aclrtMallocHost((void **)(&biasHost), biasFileSize);
        aclrtMalloc((void **)&biasDevice, biasFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
        ReadFile("./input/bias_gm.bin", biasFileSize, biasHost, biasFileSize);
        aclrtMemcpy(biasDevice, biasFileSize, biasHost, biasFileSize, ACL_MEMCPY_HOST_TO_DEVICE);
    }
    
    uint8_t* yHost = nullptr;
    uint8_t* yDevice = nullptr;
    aclrtMallocHost((void **)(&yHost), yFileSize);
    aclrtMalloc((void **)&yDevice, yFileSize, ACL_MEM_MALLOC_HUGE_FIRST);

    matmulOp(x1Device, x2Device, xs1Device, xs2Device, yDevice, biasDevice, m, n, k, stream);
    aclrtSynchronizeStream(stream);

    aclrtMemcpy(yHost, yFileSize, yDevice, yFileSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output.bin", yHost, yFileSize);

    if (IS_BIAS) {
        aclrtFree(biasDevice);
        aclrtFreeHost(biasHost);
    }
    
    aclrtFree(x1Device);
    aclrtFreeHost(x1Host);
    aclrtFree(x2Device);
    aclrtFreeHost(x2Host);
    aclrtFree(xs1Device);
    aclrtFreeHost(xs1Host);
    aclrtFree(xs2Device);
    aclrtFreeHost(xs2Host);
    aclrtFree(yDevice);
    aclrtFreeHost(yHost);
    
    testAclDeInit(context, stream, deviceId);
}

int32_t main(int32_t argc, const char* args[])
{
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    int64_t M = 32;
    int64_t N = 128;
    int64_t K = 30;
    int sK = static_cast<int>((K + SCALE_CEIL_NUMBER - 1) / SCALE_CEIL_NUMBER * SCALE_NUMBER);
    
    MatrixFileSize matrixFileSize;
    matrixFileSize.x1FileSize = static_cast<size_t>(M * K) * sizeof(uint8_t);
    matrixFileSize.x2FileSize = static_cast<size_t>(K * N) * sizeof(uint8_t);
    matrixFileSize.xs1FileSize = static_cast<size_t>(M * sK) * sizeof(uint8_t);
    matrixFileSize.xs2FileSize = static_cast<size_t>(sK * N) * sizeof(uint8_t);
    matrixFileSize.yFileSize = static_cast<size_t>(M * N) * sizeof(float);
    matrixFileSize.biasFileSize = static_cast<size_t>(1 * N) * sizeof(float);
    
    testMatmul(M, N, K, matrixFileSize);
    
    return 0;
}