/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/


#include "kernel_operator.h"
#include "lib/matmul_intf.h"
#include "tiling/tiling_api.h"
#include "tiling/platform/platform_ascendc.h"
#include "data_utils.h"
#include "kernel_tiling/kernel_tiling.h"
#include "acl/acl.h"

using namespace matmul;
using namespace std;

__aicore__ inline uint32_t Ceiling(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

/**
 * @brief  Copy tiling data to TCubeTiling ptr from tiling gm addr.
 * @param  tiling: TCubeTiling ptr which needs to copy tiling data.
 * @param  tilingGM: tiling gm addr.
 * @retval None
 */
__aicore__ inline void CopyTiling(TCubeTiling* tiling, GM_ADDR tilingGM)
{
    uint32_t* ptr = reinterpret_cast<uint32_t*>(tiling);
    auto tiling32 = reinterpret_cast<__gm__ uint32_t*>(tilingGM);

    for (uint32_t i = 0; i < sizeof(TCubeTiling) / sizeof(uint32_t); i++, ptr++) { *ptr = *(tiling32 + i); }
    return;
}

template <typename AType, typename BType, typename CType, typename BiasType>
class MatmulLeakyKernel {
public:
    __aicore__ inline MatmulLeakyKernel(){};
    __aicore__ inline void Init(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c, GM_ADDR workspace,
                                const TCubeTiling& tiling, AscendC::TPipe* pipe);
    __aicore__ inline void Process();

    __aicore__ inline void MatmulCompute();
    __aicore__ inline void LeakyReluCompute(uint32_t count);
    __aicore__ inline void CopyOut(uint32_t count);
    __aicore__ inline void CalcOffset(int32_t blockIdx, const TCubeTiling& tiling, int32_t& offsetA, int32_t& offsetB,
                                      int32_t& offsetC, int32_t& offsetBias);

    Matmul<MatmulType<AscendC::TPosition::GM, CubeFormat::ND, AType>,
           MatmulType<AscendC::TPosition::GM, CubeFormat::ND, BType>,
           MatmulType<AscendC::TPosition::VECIN, CubeFormat::ND, CType>,
           MatmulType<AscendC::TPosition::GM, CubeFormat::ND, BiasType>>
        matmulObj;

    AscendC::GlobalTensor<AType> aGlobal;
    AscendC::GlobalTensor<BType> bGlobal;
    AscendC::GlobalTensor<CType> cGlobal;
    AscendC::GlobalTensor<BiasType> biasGlobal;
    AscendC::GlobalTensor<CType> workspaceGlobal;
    AscendC::LocalTensor<CType> reluInLocal;
    TCubeTiling tiling;
    AscendC::TQue<AscendC::TPosition::VECIN, 1> reluInQueue;
    AscendC::TQue<AscendC::TPosition::VECOUT, 1> reluOutQueue;
    uint32_t splitRowNums = 0;
    uint32_t splitRowSize = 0;
};

/**
 * @brief  Set matmulLeaky input and output gm addr of current core.
 * @param  a: A matrix gm addr.
 * @param  b: B matrix gm addr.
 * @param  bias: Bias gm addr.
 * @param  c: C matrix gm addr.
 * @param  workspace: Temporary gm space addr required by matmul calc.
 * @param  tiling: matmul tiling data.
 * @param  pipe: Global memory and sync management TPipe object.
 * @retval None
 */
template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void
MatmulLeakyKernel<AType, BType, CType, BiasType>::Init(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c, GM_ADDR workspace,
                                                       const TCubeTiling& tiling, AscendC::TPipe* pipe)
{
    this->tiling = tiling;
    splitRowNums = 4;
    splitRowSize = tiling.baseM / splitRowNums;
    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ AType*>(a), tiling.M * tiling.Ka);
    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ BType*>(b), tiling.Kb * tiling.N);
    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ CType*>(c), tiling.M * tiling.N);
    biasGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ BiasType*>(bias), tiling.N);
    workspaceGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ CType*>(workspace), tiling.M * tiling.N);

    int32_t offsetA, offsetB, offsetC, offsetBias;
    CalcOffset(AscendC::GetBlockIdx(), tiling, offsetA, offsetB, offsetC,
               offsetBias); // Calculate the gm offset based on the blockidx.
    aGlobal = aGlobal[offsetA];
    bGlobal = bGlobal[offsetB];
    cGlobal = cGlobal[offsetC];
    biasGlobal = biasGlobal[offsetBias];
    workspaceGlobal = workspaceGlobal[GetBlockIdx() * tiling.singleCoreM * tiling.singleCoreN];
    pipe->InitBuffer(reluInQueue, 1, tiling.baseM * tiling.baseN * sizeof(CType));  // Init relu input queue.
    pipe->InitBuffer(reluOutQueue, 1, splitRowSize * tiling.baseN * sizeof(CType)); // Init relu output queue.
}

/**
 * @brief  Main process of matmul calculation
 * @retval None
 */
template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulLeakyKernel<AType, BType, CType, BiasType>::Process()
{
    matmulObj.SetWorkspace(workspaceGlobal);
    matmulObj.SetTensorA(aGlobal);
    matmulObj.SetTensorB(bGlobal);
    matmulObj.SetBias(biasGlobal);
    matmulObj.template Iterate<false>(); // Sync is set false means async, this scene will run while(Iterate).
    for (int i = 0; i < tiling.singleCoreM * tiling.singleCoreN / (tiling.baseM * tiling.baseN); ++i) {
        MatmulCompute();                          // Get matmul compute result.
        reluInLocal = reluInQueue.DeQue<CType>(); // wait matmul compute result finish.
        for (int j = 0; j < splitRowNums; ++j) {
            LeakyReluCompute(j);           // Compute leakyRelu.
            CopyOut(i * splitRowNums + j); // Copy leakyRelu out result to GM.
        }
        reluInQueue.FreeTensor(reluInLocal);
    }
    matmulObj.End();
}

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulLeakyKernel<AType, BType, CType, BiasType>::MatmulCompute()
{
    reluInLocal = reluInQueue.AllocTensor<CType>();
    matmulObj.template GetTensorC<false>(reluInLocal, false, true);
    reluInQueue.EnQue(reluInLocal);
}

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulLeakyKernel<AType, BType, CType, BiasType>::LeakyReluCompute(uint32_t count)
{
    auto reluOutLocal = reluOutQueue.AllocTensor<CType>();
    LeakyRelu(reluOutLocal, reluInLocal[count * splitRowSize * tiling.baseN], (CType)0.001,
              splitRowSize * tiling.baseN);
    reluOutQueue.EnQue(reluOutLocal);
}

/**
 * @brief  Copy leakyRelu out result to GM.
 * @param  count: Iterate count.
 * @retval None
 */
template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulLeakyKernel<AType, BType, CType, BiasType>::CopyOut(uint32_t count)
{
    auto reluOutLocal = reluOutQueue.DeQue<CType>(); // wait relu compute result finish.
    const uint32_t roundM = tiling.singleCoreM / splitRowSize;
    const uint32_t roundN = tiling.singleCoreN / tiling.baseN;
    uint32_t startOffset = (count % roundM * splitRowSize * tiling.N + count / roundM * tiling.baseN);
    AscendC::DataCopyParams copyParam = {
        (uint16_t)splitRowSize, (uint16_t)(tiling.baseN * sizeof(CType) / AscendC::DEFAULT_C0_SIZE), 0,
        (uint16_t)((tiling.N - tiling.baseN) * sizeof(CType) / AscendC::DEFAULT_C0_SIZE)};
    DataCopy(cGlobal[startOffset], reluOutLocal, copyParam);
    reluOutQueue.FreeTensor(reluOutLocal);
}

/**
 * @brief  Calculate the gm offset based on the blockidx.
 * @param  blockIdx: Current Core blockidx.
 * @param  tiling: Matmul tiling data.
 * @param  offsetA: Gm offset of A matrix.
 * @param  offsetB: Gm offset of B matrix.
 * @param  offsetC: Gm offset of C matrix.
 * @param  offsetBias: Gm offset of Bias matrix.
 * @retval None
 */
template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void
MatmulLeakyKernel<AType, BType, CType, BiasType>::CalcOffset(int32_t blockIdx, const TCubeTiling& tiling,
                                                             int32_t& offsetA, int32_t& offsetB, int32_t& offsetC,
                                                             int32_t& offsetBias)
{
    auto mSingleBlocks = Ceiling(tiling.M, tiling.singleCoreM);
    auto mCoreIndx = blockIdx % mSingleBlocks;
    auto nCoreIndx = blockIdx / mSingleBlocks;

    offsetA = mCoreIndx * tiling.Ka * tiling.singleCoreM;
    offsetB = nCoreIndx * tiling.singleCoreN;
    offsetC = mCoreIndx * tiling.N * tiling.singleCoreM + nCoreIndx * tiling.singleCoreN;
    offsetBias = nCoreIndx * tiling.singleCoreN;
}

/**
 * @brief  matmul_leakyrelu kernel function entry
 * @param  a: A matrix gm addr.
 * @param  b: B matrix gm addr.
 * @param  bias: Bias gm addr.
 * @param  c: Out gm addr.
 * @param  workspace: Temporary gm space addr required by matmul calc.
 * @param  tilingGm: Tiling data addr.
 * @retval None
 */
extern "C" __global__ __aicore__ void matmul_leakyrelu_custom(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c,
                                                              GM_ADDR __kfc_workspace__ workspace, GM_ADDR tilingGm)
{
    AscendC::TPipe pipe;
    TCubeTiling tiling;
    CopyTiling(&tiling, tilingGm);

    MatmulLeakyKernel<half, half, float, float> matmulLeakyKernel;
    matmulLeakyKernel.Init(a, b, bias, c, workspace, tiling, &pipe);
    REGIST_MATMUL_OBJ(&pipe, GetSysWorkSpacePtr(), matmulLeakyKernel.matmulObj, &matmulLeakyKernel.tiling);
    matmulLeakyKernel.Process();
}

/**
 * @brief  Generate matmul tiling.
 * @param  socVersion: Platform socversion.
 * @param  tilingBuf data buffer.
 */
void GenerateTiling(platform_ascendc::PlatformAscendC* ascendcPlatform, uint8_t* tilingBuf)
{
    int M = 1024;
    int N = 640;
    int K = 256;

    matmul_tiling::TPosition leftPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat leftFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType leftDtype = matmul_tiling::DataType::DT_FLOAT16;
    bool isTransA = false;

    matmul_tiling::TPosition rightPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat rightFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType rightDtype = matmul_tiling::DataType::DT_FLOAT16;
    bool isTransB = false;

    matmul_tiling::TPosition resultPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat resultFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType resultDtype = matmul_tiling::DataType::DT_FLOAT;

    matmul_tiling::TPosition biasPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat biasFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType biasDtype = matmul_tiling::DataType::DT_FLOAT;
    bool isBias = true;

    int usedCoreNum = 2;
    int baseM = 256;
    int baseN = 128;

    optiling::TCubeTiling tilingData;
    matmul_tiling::MultiCoreMatmulTiling tilingApi(*ascendcPlatform);

    tilingApi.SetDim(usedCoreNum); // Set the number of cores that participate in multi-core computaion is 2.
    tilingApi.SetAType(leftPosition, leftFormat, leftDtype, isTransA);
    tilingApi.SetBType(rightPosition, rightFormat, rightDtype, isTransB);
    tilingApi.SetCType(resultPosition, resultFormat, resultDtype);
    tilingApi.SetBiasType(biasPosition, biasFormat, biasDtype);

    tilingApi.SetOrgShape(M, N, K);
    tilingApi.SetShape(M, N, K);
    tilingApi.SetBias(isBias);
    tilingApi.SetTraverse(matmul_tiling::MatrixTraverse::FIRSTM); // Set the matmul travse is FIRSTM.
    tilingApi.SetFixSplit(baseM, baseN, -1);                      // Set the fixed baseM=256, baseN=128.
    tilingApi.SetBufferSpace(-1, -1, -1);

    int64_t res = tilingApi.GetTiling(tilingData); // Get matmul tiling data.
    tilingData.set_stepM(1);                       // Set the matmul tiling stepM=1.
    tilingData.set_stepN(1);                       // Set the matmul tiling stepN=1.
    if (res == -1) {
        std::cout << "gen tiling failed" << std::endl;
    }
    uint32_t tcubeTilingSize = tilingData.GetDataSize();
    tilingData.SaveToBuffer(tilingBuf, tcubeTilingSize);
    return;
}

int32_t main(int32_t argc, char* argv[])
{
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    uint32_t M = 1024;
    uint32_t N = 640;
    uint32_t K = 256;
    size_t aFileSize = M * K * sizeof(int16_t);
    size_t bFileSize = K * N * sizeof(int16_t);
    size_t cFileSize = M * N * sizeof(float);
    size_t biasFileSize = N * sizeof(float);
    size_t tilingFileSize = sizeof(TCubeTiling);
    size_t userWorkspaceSize = M * N * sizeof(float);
    size_t systemWorkspaceSize = static_cast<size_t>(ascendcPlatform->GetLibApiWorkSpaceSize());
    size_t workspaceSize = userWorkspaceSize + systemWorkspaceSize;
    uint32_t numBlocks = 1;
    uint8_t* tilingBuf = (uint8_t*)malloc(tilingFileSize);
    GenerateTiling(ascendcPlatform, tilingBuf);

#ifdef ASCENDC_CPU_DEBUG
    uint8_t* a = (uint8_t*)AscendC::GmAlloc(aFileSize);
    uint8_t* b = (uint8_t*)AscendC::GmAlloc(bFileSize);
    uint8_t* bias = (uint8_t*)AscendC::GmAlloc(biasFileSize);
    uint8_t* c = (uint8_t*)AscendC::GmAlloc(cFileSize);
    uint8_t* tiling = (uint8_t*)AscendC::GmAlloc(tilingFileSize);
    uint8_t* workspace = (uint8_t*)AscendC::GmAlloc(workspaceSize);

    ReadFile("./input/x1_gm.bin", aFileSize, a, aFileSize);
    ReadFile("./input/x2_gm.bin", bFileSize, b, bFileSize);
    ReadFile("./input/bias.bin", biasFileSize, bias, biasFileSize);
    memcpy_s(tiling, tilingFileSize, tilingBuf, tilingFileSize);
    ICPU_RUN_KF(matmul_leakyrelu_custom, numBlocks, a, b, bias, c, workspace, tiling);

    WriteFile("./output/output.bin", c, cFileSize);
    AscendC::GmFree((void*)a);
    AscendC::GmFree((void*)b);
    AscendC::GmFree((void*)bias);
    AscendC::GmFree((void*)c);
    AscendC::GmFree((void*)tiling);
    AscendC::GmFree((void*)workspace);
#else
    aclInit(nullptr);
    int32_t deviceId = 0;
    aclrtSetDevice(deviceId);
    aclrtStream stream = nullptr;
    aclrtCreateStream(&stream);

    uint8_t* inputAHost;
    uint8_t* inputADevice;
    aclrtMallocHost((void**)(&inputAHost), aFileSize);
    aclrtMalloc((void**)&inputADevice, aFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x1_gm.bin", aFileSize, inputAHost, aFileSize);
    aclrtMemcpy(inputADevice, aFileSize, inputAHost, aFileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* inputBHost;
    uint8_t* inputBDevice;
    aclrtMallocHost((void**)(&inputBHost), bFileSize);
    aclrtMalloc((void**)&inputBDevice, bFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x2_gm.bin", bFileSize, inputBHost, bFileSize);
    aclrtMemcpy(inputBDevice, bFileSize, inputBHost, bFileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* outputCHost;
    uint8_t* outputCDevice;
    aclrtMallocHost((void**)(&outputCHost), cFileSize);
    aclrtMalloc((void**)&outputCDevice, cFileSize, ACL_MEM_MALLOC_HUGE_FIRST);

    uint8_t* inputBiasHost;
    uint8_t* inputBiasDevice;
    aclrtMallocHost((void**)(&inputBiasHost), biasFileSize);
    aclrtMalloc((void**)&inputBiasDevice, biasFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/bias.bin", biasFileSize, inputBiasHost, biasFileSize);
    aclrtMemcpy(inputBiasDevice, biasFileSize, inputBiasHost, biasFileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* tilingHost;
    uint8_t* tilingDevice;
    aclrtMallocHost((void**)(&tilingHost), tilingFileSize);
    aclrtMalloc((void**)&tilingDevice, tilingFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMemcpy(tilingHost, tilingFileSize, tilingBuf, tilingFileSize, ACL_MEMCPY_HOST_TO_HOST);
    aclrtMemcpy(tilingDevice, tilingFileSize, tilingHost, tilingFileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* workspaceDevice;
    aclrtMalloc((void**)&workspaceDevice, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);

    matmul_leakyrelu_custom<<<numBlocks, nullptr, stream>>>(inputADevice, inputBDevice, inputBiasDevice, outputCDevice,
                                                           workspaceDevice, tilingDevice);

    aclrtSynchronizeStream(stream);

    aclrtFree(inputADevice);
    aclrtFreeHost(inputAHost);
    aclrtFree(inputBDevice);
    aclrtFreeHost(inputBHost);
    aclrtMemcpy(outputCHost, cFileSize, outputCDevice, cFileSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output.bin", outputCHost, cFileSize);
    aclrtFree(outputCDevice);
    aclrtFreeHost(outputCHost);
    aclrtFree(inputBiasDevice);
    aclrtFreeHost(inputBiasHost);
    aclrtFree(tilingDevice);
    aclrtFreeHost(tilingHost);
    aclrtFree(workspaceDevice);

    aclrtDestroyStream(stream);
    aclrtResetDevice(deviceId);
    aclFinalize();
#endif
    free(tilingBuf);
    return 0;
}