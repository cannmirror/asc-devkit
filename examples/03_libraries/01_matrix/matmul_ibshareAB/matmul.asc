/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/


#include "kernel_operator.h"
#include "lib/matmul_intf.h"
#include "tiling/tiling_api.h"
#include "data_utils.h"
#include "kernel_tiling/kernel_tiling.h"
#include "tiling/platform/platform_ascendc.h"
#include "acl/acl.h"

using namespace matmul;
using namespace std;

constexpr uint16_t CYCLENUMS = 10000;
constexpr float PRECENT = 100;
constexpr bool isABshare = true;

__aicore__ inline void CopyTiling(TCubeTiling* tiling, GM_ADDR tilingGM) // copy tiling info
{
    uint32_t* ptr = reinterpret_cast<uint32_t*>(tiling);
    auto tiling32 = reinterpret_cast<__gm__ uint32_t*>(tilingGM);

    for (uint32_t i = 0; i < sizeof(TCubeTiling) / sizeof(uint32_t); i++, ptr++) { *ptr = *(tiling32 + i); }
    return;
}

template <typename AType, typename BType, typename CType>
class MatmulABshareKernel {
public:
    __aicore__ inline MatmulABshareKernel(){};
    __aicore__ inline void Init(GM_ADDR a, GM_ADDR b, GM_ADDR c, GM_ADDR workspace, const TCubeTiling& tiling,
                                AscendC::TPipe* pipe);
    __aicore__ inline void Process(AscendC::TPipe* pipe);
    __aicore__ inline void CalcOffset(int32_t blockIdx, const TCubeTiling& tiling, int32_t& offsetA, int32_t& offsetB,
                                      int32_t& offsetC);

    Matmul<MatmulType<AscendC::TPosition::GM, CubeFormat::ND, AType, false, LayoutMode::NONE, isABshare>,
           MatmulType<AscendC::TPosition::GM, CubeFormat::ND, BType, false, LayoutMode::NONE, isABshare>,
           MatmulType<AscendC::TPosition::VECIN, CubeFormat::ND, CType>>
        matmulObj;

    AscendC::GlobalTensor<AType> aGlobal;
    AscendC::GlobalTensor<BType> bGlobal;
    AscendC::GlobalTensor<CType> cGlobal;
    TCubeTiling tiling;
};

template <typename AType, typename BType, typename CType>
__aicore__ inline void MatmulABshareKernel<AType, BType, CType>::Init(GM_ADDR a, GM_ADDR b, GM_ADDR c,
                                                                      GM_ADDR workspace, const TCubeTiling& tiling,
                                                                      AscendC::TPipe* pipe)
{
    this->tiling = tiling;
    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ AType*>(a), tiling.M * tiling.Ka);
    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ BType*>(b), tiling.Kb * tiling.N);
    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ CType*>(c), tiling.M * tiling.N);

    int32_t offsetA, offsetB, offsetC;
    CalcOffset(AscendC::GetBlockIdx(), tiling, offsetA, offsetB, offsetC); // calculate offset
    aGlobal = aGlobal[offsetA];
    bGlobal = bGlobal[offsetB];
    cGlobal = cGlobal[offsetC];
}

template <typename AType, typename BType, typename CType>
__aicore__ inline void MatmulABshareKernel<AType, BType, CType>::Process(AscendC::TPipe* pipe)
{
    matmulObj.Init(&tiling);
    matmulObj.SetTensorA(aGlobal);
    matmulObj.SetTensorB(bGlobal);
    matmulObj.SetSingleShape(tiling.M, tiling.N, tiling.Ka); // set matmul single-core process shape
    matmulObj.IterateAll(cGlobal);
    matmulObj.End();
}

template <typename AType, typename BType, typename CType>
__aicore__ inline void MatmulABshareKernel<AType, BType, CType>::CalcOffset(int32_t blockIdx, const TCubeTiling& tiling,
                                                                            int32_t& offsetA, int32_t& offsetB,
                                                                            int32_t& offsetC)
{
    offsetA = 0;
    offsetB = 0;
    offsetC = 0;
}

extern "C" __global__ __aicore__ void matmul_ABshare_custom(GM_ADDR a, GM_ADDR b, GM_ADDR c,
                                                            GM_ADDR __kfc_workspace__ workspace, GM_ADDR tilingGm)
{
    AscendC::TPipe pipe;
    TCubeTiling tiling;
    CopyTiling(&tiling, tilingGm);

    MatmulABshareKernel<half, half, float> MatmulABshareKernel;
    MatmulABshareKernel.Init(a, b, c, workspace, tiling, &pipe);
    REGIST_MATMUL_OBJ(&pipe, GetSysWorkSpacePtr(), MatmulABshareKernel.matmulObj, &MatmulABshareKernel.tiling);
    MatmulABshareKernel.Process(&pipe);
}

uint8_t* GetTilingBuf(optiling::TCubeTiling* tilingData)
{
    uint32_t tilingSize = tilingData->GetDataSize();
    uint8_t* buf = (uint8_t*)malloc(tilingSize);
    tilingData->SaveToBuffer(buf, tilingSize);
    return buf;
}

uint8_t* GenerateTiling(platform_ascendc::PlatformAscendC* ascendcPlatform)
{
    int M = 128;
    int N = 256;
    int K = 384;

    matmul_tiling::TPosition leftPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat leftFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType leftDtype = matmul_tiling::DataType::DT_FLOAT16;
    bool isTransA = false;

    matmul_tiling::TPosition rightPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat rightFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType rightDtype = matmul_tiling::DataType::DT_FLOAT16;
    bool isTransB = false;

    matmul_tiling::TPosition resultPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat resultFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType resultDtype = matmul_tiling::DataType::DT_FLOAT;

    matmul_tiling::TPosition biasPosition = matmul_tiling::TPosition::GM;
    matmul_tiling::CubeFormat biasFormat = matmul_tiling::CubeFormat::ND;
    matmul_tiling::DataType biasDtype = matmul_tiling::DataType::DT_FLOAT;
    bool isBias = false;

    int usedCoreNum = 2;
    int baseM = 32;
    int baseN = 64;

    optiling::TCubeTiling tilingData;
    matmul_tiling::MultiCoreMatmulTiling tilingApi(*ascendcPlatform);

    tilingApi.SetDim(usedCoreNum);
    tilingApi.SetAType(leftPosition, leftFormat, leftDtype, isTransA);
    tilingApi.SetBType(rightPosition, rightFormat, rightDtype, isTransB);
    tilingApi.SetCType(resultPosition, resultFormat, resultDtype);
    tilingApi.SetBiasType(biasPosition, biasFormat, biasDtype);

    tilingApi.SetOrgShape(M, N, K);
    tilingApi.SetShape(M, N, K);
    tilingApi.SetBias(isBias);
    tilingApi.SetTraverse(matmul_tiling::MatrixTraverse::FIRSTM);
    tilingApi.SetFixSplit(baseM, baseN, -1);
    tilingApi.SetBufferSpace(-1, -1, -1);

    int64_t res = tilingApi.GetTiling(tilingData);
    tilingData.set_stepM(1);
    tilingData.set_stepN(1);
    if (res == -1) {
        std::cout << "gen tiling failed" << std::endl;
    }
    return GetTilingBuf(&tilingData);
}

int32_t main(int32_t argc, char* argv[])
{
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    size_t aFileSize = 49152 * sizeof(int16_t);
    size_t bFileSize = 98304 * sizeof(int16_t);
    size_t cFileSize = 32768 * sizeof(float);
    size_t tilingFileSize = sizeof(TCubeTiling);
    size_t userWorkspaceSize = 0;
    size_t systemWorkspaceSize = static_cast<size_t>(ascendcPlatform->GetLibApiWorkSpaceSize());
    size_t workspaceSize = userWorkspaceSize + systemWorkspaceSize;
    uint32_t blockDim = 1;

    aclInit(nullptr);
    int32_t deviceId = 0;
    aclrtSetDevice(deviceId);
    aclrtStream stream = nullptr;
    aclrtCreateStream(&stream);

    uint8_t* inputAHost;
    uint8_t* inputADevice;
    aclrtMallocHost((void**)(&inputAHost), aFileSize);
    aclrtMalloc((void**)&inputADevice, aFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x1_gm.bin", aFileSize, inputAHost, aFileSize);
    aclrtMemcpy(inputADevice, aFileSize, inputAHost, aFileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* inputBHost;
    uint8_t* inputBDevice;
    aclrtMallocHost((void**)(&inputBHost), bFileSize);
    aclrtMalloc((void**)&inputBDevice, bFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x2_gm.bin", bFileSize, inputBHost, bFileSize);
    aclrtMemcpy(inputBDevice, bFileSize, inputBHost, bFileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* outputCHost;
    uint8_t* outputCDevice;
    aclrtMallocHost((void**)(&outputCHost), cFileSize);
    aclrtMalloc((void**)&outputCDevice, cFileSize, ACL_MEM_MALLOC_HUGE_FIRST);

    uint8_t* tilingHost;
    uint8_t* tilingDevice;
    aclrtMallocHost((void**)(&tilingHost), tilingFileSize);
    aclrtMalloc((void**)&tilingDevice, tilingFileSize, ACL_MEM_MALLOC_HUGE_FIRST);

    aclrtMemcpy(tilingHost, tilingFileSize, GenerateTiling(ascendcPlatform), tilingFileSize, ACL_MEMCPY_HOST_TO_HOST);
    aclrtMemcpy(tilingDevice, tilingFileSize, tilingHost, tilingFileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* workspaceDevice;
    aclrtMalloc((void**)&workspaceDevice, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);

    // Execute Matmul_ABshare_custom
    auto start = std::chrono::steady_clock::now();
    for (int i = 0; i < CYCLENUMS; ++i) {
        matmul_ABshare_custom<<<blockDim, nullptr, stream>>>(inputADevice, inputBDevice, outputCDevice, workspaceDevice,
                                                             tilingDevice);
    };
    aclrtSynchronizeStream(stream);
    auto end = std::chrono::steady_clock::now();
    auto diff_ABshare_custom = ReportTime(start, end);
    INFO_LOG("Matmul_ABshare_custom Execute time [%f]", diff_ABshare_custom);
    aclrtMemcpy(outputCHost, cFileSize, outputCDevice, cFileSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output_ABshare.bin", outputCHost, cFileSize);

    aclrtFree(inputADevice);
    aclrtFreeHost(inputAHost);
    aclrtFree(inputBDevice);
    aclrtFreeHost(inputBHost);
    aclrtFree(outputCDevice);
    aclrtFreeHost(outputCHost);
    aclrtFree(tilingDevice);
    aclrtFreeHost(tilingHost);
    aclrtFree(workspaceDevice);
    aclrtDestroyStream(stream);
    aclrtResetDevice(deviceId);
    aclFinalize();
    return 0;
}