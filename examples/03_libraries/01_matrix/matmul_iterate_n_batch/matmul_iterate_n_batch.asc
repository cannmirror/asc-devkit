/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
* This program is free software, you can redistribute it and/or modify it under the terms and conditions of
* CANN Open Software License Agreement Version 2.0 (the "License").
* Please refer to the License for details. You may not use this file except in compliance with the License.
* THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
* INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
* See LICENSE in the root of the software repository for the full text of the License.
*/


/*!
 * \file matmul_iterate_n_batch.asc
 * \brief
 */
#include <iostream>
#include <sstream>
#include "acl/acl.h"
#include "kernel_tiling/kernel_tiling.h"
#include "tiling/platform/platform_ascendc.h"
#include "register/tilingdata_base.h"
#include "tiling/tiling_api.h"
#include "kernel_operator.h"
#include "lib/matmul_intf.h"
#include "kernel_operator.h"
#include "lib/matmul_intf.h"
#include "data_utils.h"

 struct MatmulCaseParams
{
    int32_t usedCoreNum;
    int32_t m;
    int32_t n;
    int32_t k;
    bool isBias;
    bool isATrans;
    bool isBTrans;
    int32_t batchNum;
};

TCubeTiling GenerateTiling(const MatmulCaseParams& testCaseParams)
{
    TCubeTiling tilingData;
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    matmul_tiling::MultiCoreMatmulTiling cubeTiling(*ascendcPlatform);
    int32_t M = testCaseParams.m;
    int32_t N = testCaseParams.n;
    int32_t K = testCaseParams.k;
    int32_t blockDim = 1;
    bool isBias = testCaseParams.isBias;
    bool isAtrans = testCaseParams.isATrans;
    bool isBtrans = testCaseParams.isBTrans;
    int32_t batchNum = testCaseParams.batchNum;

    cubeTiling.SetDim(blockDim);
    cubeTiling.SetAType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT16, isAtrans);
    cubeTiling.SetBType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT16, isBtrans);
    cubeTiling.SetCType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT);
    cubeTiling.SetBiasType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
        matmul_tiling::DataType::DT_FLOAT);
    cubeTiling.SetOrgShape(M, N, K);
    cubeTiling.SetShape(M, N, K);
    cubeTiling.EnableBias(isBias);
    cubeTiling.SetBufferSpace(-1, -1, -1);
    cubeTiling.SetALayout(1, M, 1, batchNum, K);
    cubeTiling.SetBLayout(1, N, 1, batchNum, K);
    cubeTiling.SetCLayout(1, M, 1, batchNum, N);
    cubeTiling.SetBatchNum(batchNum);
    if (cubeTiling.GetTiling(tilingData) == -1) {
        std::cout << "Generate tiling failed." << std::endl;
        return {};
    }
    return tilingData;
}

constexpr MatmulConfigMode configMode = MatmulConfigMode::CONFIG_NORM;
constexpr MatmulBatchParams batchParams = {
  true, BatchMode::BATCH_LESS_THAN_L1, false /* isNBatch, batchMode, isBiasBatch */
};
constexpr MatmulConfig CFG_MM = GetMMConfig<configMode>(batchParams);
constexpr bool IS_SYNCH = true; //synchronous or asynchronous for getting matrix C

template <class A_TYPE, class B_TYPE, class C_TYPE, class BIAS_TYPE>
class BatchMatmulKernel {
    public:
        __aicore__ inline BatchMatmulKernel(){};
        __aicore__ inline void Init(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c, GM_ADDR workspace, const TCubeTiling& tiling);
        __aicore__ inline void Process(AscendC::TPipe* pipe, int32_t batchA, int32_t batchB);
        AscendC::Matmul<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE, CFG_MM> matmulObj;
    private:
        using aType = typename A_TYPE::T;
        using bType = typename B_TYPE::T;
        using cType = typename C_TYPE::T;
        using biasType = typename BIAS_TYPE::T;
        AscendC::GlobalTensor<aType> aGlobal;
        AscendC::GlobalTensor<bType> bGlobal;
        AscendC::GlobalTensor<cType> cGlobal;
        AscendC::GlobalTensor<biasType> biasGlobal;
        AscendC::GlobalTensor<cType> gm_workspace;
        TCubeTiling tiling;
};

template <class A_TYPE, class B_TYPE, class C_TYPE, class BIAS_TYPE>
__aicore__ inline void BatchMatmulKernel<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE>::Init(GM_ADDR a, GM_ADDR b, GM_ADDR bias,
                                         GM_ADDR c, GM_ADDR workspace, const TCubeTiling& tiling)
{
    this->tiling = tiling;
    int32_t sizeA = tiling.ALayoutInfoB * tiling.ALayoutInfoS * tiling.ALayoutInfoN * tiling.ALayoutInfoG * tiling.ALayoutInfoD;
    int32_t sizeB = tiling.BLayoutInfoB * tiling.BLayoutInfoS * tiling.BLayoutInfoN * tiling.BLayoutInfoG * tiling.BLayoutInfoD;
    int32_t sizeC = tiling.CLayoutInfoB * tiling.CLayoutInfoS1 * tiling.CLayoutInfoN * tiling.CLayoutInfoG * tiling.CLayoutInfoS2;
    int32_t sizeBias = tiling.CLayoutInfoN * tiling.CLayoutInfoG * tiling.CLayoutInfoS2;

    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ aType*>(a), sizeA);
    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ bType*>(b), sizeB);
    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ cType*>(c), sizeC);
    biasGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ biasType*>(bias), sizeBias);
    gm_workspace.SetGlobalBuffer(reinterpret_cast<__gm__ cType*>(workspace), sizeC);

    int32_t offsetA = 0;
    int32_t offsetB = 0;
    int32_t offsetC = 0;
    int32_t offsetBias = 0;
    aGlobal = aGlobal[offsetA];
    bGlobal = bGlobal[offsetB];
    cGlobal = cGlobal[offsetC];
    biasGlobal = biasGlobal[offsetBias];
    gm_workspace = gm_workspace[offsetC];

    if (GetSysWorkSpacePtr() == nullptr) {
        return;
    }
}

template <class A_TYPE, class B_TYPE, class C_TYPE, class BIAS_TYPE>
__aicore__ inline void BatchMatmulKernel<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE>::Process(AscendC::TPipe* pipe, int32_t batchA, int32_t batchB)
{
    matmulObj.SetTensorA(aGlobal, false);
    matmulObj.SetTensorB(bGlobal, true); // B transpose
    if (tiling.isBias) {
        matmulObj.SetBias(biasGlobal);
    }

    int32_t g_lay = tiling.ALayoutInfoG > tiling.BLayoutInfoG ? tiling.ALayoutInfoG : tiling.BLayoutInfoG;
    int32_t nNum = tiling.ALayoutInfoB * tiling.ALayoutInfoN * g_lay / tiling.BatchNum;

    if constexpr(IS_SYNCH){
        //synchronous UB
        int32_t sizeC = tiling.CLayoutInfoB * tiling.CLayoutInfoS1 * tiling.CLayoutInfoN * tiling.CLayoutInfoG * tiling.CLayoutInfoS2 * sizeof(cType);

        AscendC::TQue<AscendC::TPosition::VECIN, 2> resultCMatrix;
        AscendC::LocalTensor<cType> bufferC;

        pipe->InitBuffer(resultCMatrix, 1, sizeC);
        bufferC = resultCMatrix.AllocTensor<cType>();

        matmulObj.SetWorkspace(gm_workspace);
        matmulObj.IterateNBatch(nNum, batchA, batchB, false);
        DataCopy(bufferC, gm_workspace, sizeC / sizeof(cType));

        int32_t eventID = static_cast<int32_t>(GetTPipePtr()->FetchEventID(AscendC::HardEvent::MTE2_MTE3));
        AscendC::SetFlag<AscendC::HardEvent::MTE2_MTE3>(eventID);
        AscendC::WaitFlag<AscendC::HardEvent::MTE2_MTE3>(eventID);

        resultCMatrix.EnQue(bufferC);
        bufferC = resultCMatrix.DeQue<cType>();
        DataCopy(cGlobal, bufferC, sizeC / sizeof(cType));
        resultCMatrix.FreeTensor(bufferC);
    }
    else {
        //asynchronous GM 
        matmulObj.SetWorkspace(cGlobal);
        matmulObj.template IterateNBatch<false>(nNum, batchA, batchB, false);
        for(int32_t j = 0; j < nNum; ++j){
            matmulObj.template GetBatchTensorC<false>(batchA, batchB, false);
        }
    }
}

__aicore__ inline void CopyTiling(TCubeTiling* tiling, GM_ADDR tilingGM)
{
    uint32_t* ptr = reinterpret_cast<uint32_t*>(tiling);
    auto tiling32 = reinterpret_cast<__gm__ uint32_t*>(tilingGM);

    for (int32_t i = 0; i < sizeof(TCubeTiling) / sizeof(uint32_t); i++, ptr++) {
      *ptr = *(tiling32 + i);
    }
    return;
}

extern "C" __global__ __aicore__ void matmul_iterate_n_batch_custom(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c,
 __kfc_workspace__ GM_ADDR workspace, GM_ADDR tilingGm)
{
    // prepare tiling
    TCubeTiling tiling;
    CopyTiling(&tiling, tilingGm);
    // define matmul kernel
    using A_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, half, false, LayoutMode::BSNGD>;
    using B_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, half, true, LayoutMode::BSNGD>;
    using C_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, float, false, LayoutMode::BSNGD>;
    using BIAS_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, float>;
    BatchMatmulKernel<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE> batchMatmulKernel;
    AscendC::TPipe pipe;
    REGIST_MATMUL_OBJ(&pipe, GetSysWorkSpacePtr(), batchMatmulKernel.matmulObj, &tiling);
    KERNEL_TASK_TYPE_DEFAULT(KERNEL_TYPE_MIX_AIC_1_2);
    // init matmul kernel
    batchMatmulKernel.Init(a, b, bias, c, workspace, tiling);
    // matmul kernel process
    batchMatmulKernel.Process(&pipe, 3, 3);
}

void matmul_iterate_n_batch_custom_do(uint32_t blockDim, void* stream, GM_ADDR a, GM_ADDR b, GM_ADDR bias,
                         GM_ADDR c, GM_ADDR workspace, GM_ADDR tilingGm)
{
    // invoke the kernel function through the <<<>>> symbol
    matmul_iterate_n_batch_custom<<<blockDim, nullptr, stream>>>(a, b, bias, c, workspace, tilingGm);
}

constexpr bool IS_BIAS = true;
constexpr bool IS_A_TRANS = true;
constexpr bool IS_B_TRANS = false;

struct MatrixFileSize
{
    size_t x1FileSize;
    size_t x2FileSize;
    size_t yFileSize;
    size_t biasFileSize;
};

static size_t GetSysWorkSpaceSize()
{
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    if (ascendcPlatform == nullptr) {
        return 0;
    }
    return static_cast<size_t>(ascendcPlatform->GetLibApiWorkSpaceSize());
}

void MatmulOp(uint8_t* x1, uint8_t* x2, uint8_t* y, uint8_t* bias, MatmulCaseParams testCaseParams,
    size_t yFileSize, void* stream = nullptr)
{
    // Init args
    uint8_t* workspaceDevice;

    // Query workspace size
    size_t workspaceSize = GetSysWorkSpaceSize();
    workspaceSize += yFileSize;

    // Allocate workspace on device
    aclrtMalloc((void **)&workspaceDevice, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);

    // Initialize kernel with arguments and workspace pointer
    uint8_t* tilingHost;
    uint8_t* tilingDevice;
    size_t tilingFileSize = sizeof(TCubeTiling);
    // Calculate Tiling
    const auto tilingData = GenerateTiling(testCaseParams);
    aclrtMallocHost((void **)(&tilingHost), tilingFileSize);
    aclrtMalloc((void **)&tilingDevice, tilingFileSize,
                          ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMemcpy(tilingHost, tilingFileSize, &tilingData,
                          tilingFileSize, ACL_MEMCPY_HOST_TO_HOST);
    aclrtMemcpy(tilingDevice, tilingFileSize, tilingHost,
                          tilingFileSize, ACL_MEMCPY_HOST_TO_DEVICE);
    // Launch kernel
    matmul_iterate_n_batch_custom_do(tilingData.usedCoreNum, stream, x1, x2, bias, y, workspaceDevice, tilingDevice);
    aclrtFreeHost(tilingHost);
    aclrtFree(workspaceDevice);
    aclrtFree(tilingDevice);
}

void TestAclInit(aclrtContext &context, aclrtStream &stream, int64_t &deviceId)
{
    aclInit(nullptr);
    aclrtSetDevice(deviceId);
    aclrtCreateContext(&context, deviceId);
    aclrtCreateStream(&stream);
}

void TestAclDeInit(aclrtContext &context, aclrtStream &stream, int64_t &deviceId)
{
    aclrtDestroyStream(stream);
    aclrtDestroyContext(context);
    aclrtResetDevice(deviceId);
    aclFinalize();
}

void TestMatmul(const MatmulCaseParams &testCaseParams, const MatrixFileSize& matrixFileSize)
{
    size_t x1FileSize = matrixFileSize.x1FileSize;
    size_t x2FileSize = matrixFileSize.x2FileSize;
    size_t yFileSize = matrixFileSize.yFileSize;
    size_t biasFileSize = matrixFileSize.biasFileSize;

    aclrtContext context;
    aclrtStream stream = nullptr;
    int64_t deviceId = 0;
    TestAclInit(context, stream, deviceId);

    uint8_t* x1Host;
    uint8_t* x1Device;
    aclrtMallocHost((void **)(&x1Host), x1FileSize);
    aclrtMalloc((void **)&x1Device, x1FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x1_gm.bin", x1FileSize, x1Host, x1FileSize);
    aclrtMemcpy(x1Device, x1FileSize, x1Host, x1FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* x2Host;
    uint8_t* x2Device;
    aclrtMallocHost((void **)(&x2Host), x2FileSize);
    aclrtMalloc((void **)&x2Device, x2FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x2_gm.bin", x2FileSize, x2Host, x2FileSize);
    aclrtMemcpy(x2Device, x2FileSize, x2Host, x2FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* biasHost = nullptr;
    uint8_t* biasDevice = nullptr;
    if (IS_BIAS) {
        aclrtMallocHost((void **)(&biasHost), biasFileSize);
        aclrtMalloc((void **)&biasDevice, biasFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
        ReadFile("./input/bias_gm.bin", biasFileSize, biasHost, biasFileSize);
        aclrtMemcpy(biasDevice, biasFileSize, biasHost, biasFileSize, ACL_MEMCPY_HOST_TO_DEVICE);
    }
    uint8_t* yHost = nullptr;
    uint8_t* yDevice = nullptr;
    aclrtMallocHost((void **)(&yHost), yFileSize);
    aclrtMalloc((void **)&yDevice, yFileSize, ACL_MEM_MALLOC_HUGE_FIRST);

    MatmulOp(x1Device, x2Device, yDevice, biasDevice, testCaseParams, yFileSize, stream);
    aclrtSynchronizeStream(stream);

    aclrtMemcpy(yHost, yFileSize, yDevice, yFileSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output.bin", yHost, yFileSize);

    if (IS_BIAS) {
        aclrtFree(biasDevice);
        aclrtFreeHost(biasHost);
    }
    aclrtFree(x1Device);
    aclrtFreeHost(x1Host);
    aclrtFree(x2Device);
    aclrtFreeHost(x2Host);
    aclrtFree(yDevice);
    aclrtFreeHost(yHost);
    TestAclDeInit(context, stream, deviceId);
}

int32_t main(int32_t argc, const char *args[])
{
    constexpr uint32_t M = 32;
    constexpr uint32_t N = 256;
    constexpr uint32_t K = 64;
    constexpr uint32_t B = 3;
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    MatmulCaseParams testCaseParams{static_cast<int32_t>(ascendcPlatform->GetCoreNumAic()),
        static_cast<int32_t>(M), static_cast<int32_t>(N), static_cast<int32_t>(K), // M, N, K
        IS_BIAS, IS_A_TRANS, IS_B_TRANS,
        static_cast<int32_t>(B)}; // BatchNum
    MatrixFileSize matrixFileSize;
    matrixFileSize.x1FileSize = static_cast<size_t>(testCaseParams.m * testCaseParams.k * testCaseParams.batchNum) * sizeof(uint16_t);
    matrixFileSize.x2FileSize = static_cast<size_t>(testCaseParams.k * testCaseParams.n * testCaseParams.batchNum) * sizeof(uint16_t);
    matrixFileSize.yFileSize = static_cast<size_t>(testCaseParams.m * testCaseParams.n * testCaseParams.batchNum) * sizeof(float);
    matrixFileSize.biasFileSize = static_cast<size_t>(1 * testCaseParams.n) * sizeof(float);
    TestMatmul(testCaseParams, matrixFileSize);
    return 0;
}
