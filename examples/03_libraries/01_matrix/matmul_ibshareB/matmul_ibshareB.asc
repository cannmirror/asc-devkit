/**
 * Copyright (c) 2025 Huawei Technologies Co., Ltd.
 * This program is free software, you can redistribute it and/or modify it under the terms and conditions of
 * CANN Open Software License Agreement Version 2.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

/* !
 * \file matmul_ibshareB.asc
 * \brief
 */

#include <sstream>
#include "kernel_operator.h"
#ifdef ENABLE_CUBE_ONLY
#define ASCENDC_CUBE_ONLY
#endif
#include "lib/matmul_intf.h"
#include "register/tilingdata_base.h"
#include "tiling/tiling_api.h"
#include "data_utils.h"
#include "kernel_tiling/kernel_tiling.h"
#include "tiling/platform/platform_ascendc.h"
#include "acl/acl.h"

namespace {
constexpr int32_t MIX_RATIO = 2; // AIC:AIV=1:2
constexpr bool IS_BIAS = false;
constexpr bool IS_A_TRANS = false;
constexpr bool IS_B_TRANS = false;
constexpr int32_t USED_CORE_NUM = 2; // AIC:AIV=1:2

struct MatrixFileSize {
    size_t x1FileSize;
    size_t x2FileSize;
    size_t yFileSize;
    size_t biasFileSize;
};

/**
 * @brief  Copy tiling data to TCubeTiling ptr from tiling gm addr.
 * @param  tiling: TCubeTiling ptr which needs to copy tiling data.
 * @param  tilingGM: Tiling gm addr.
 * @retval None
 */
__aicore__ inline void CopyTiling(TCubeTiling* tiling, GM_ADDR tilingGM)
{
    uint32_t* ptr = reinterpret_cast<uint32_t*>(tiling);
    auto tiling32 = reinterpret_cast<__gm__ uint32_t*>(tilingGM);

    for (int i = 0; i < sizeof(TCubeTiling) / sizeof(uint32_t); i++, ptr++) { *ptr = *(tiling32 + i); }
    return;
}

struct MatmulCaseParams {
    int32_t m;
    int32_t n;
    int32_t k;
    bool hasBias;
    bool isATrans;
    bool isBTrans;
};
} // namespace

template <typename AType, typename BType, typename CType, typename BiasType>
class MatmulKernel {
public:
    __aicore__ inline MatmulKernel(){};
    /**
     * @brief  Initialization before process.
     * @param  a: A matrix gm addr.
     * @param  b: B matrix gm addr.
     * @param  bias: Bias matrix gm addr.
     * @param  c: C matrix gm addr.
     * @param  tiling: Matmul tiling struct.
     * @param  isTransA: Whether A matrix is transposed.
     * @param  isTransB: Whether B matrix is transposed.
     * @retval None
     */
    __aicore__ inline void Init(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c, const TCubeTiling& tiling, bool isTransA,
                                bool isTransB);
    /**
     * @brief  Process matrix calculation.
     * @retval None
     */
    __aicore__ inline void Process();

    using A_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, AType>;
    using C_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, CType>;
    using BIAS_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, BiasType>;

#ifdef ENABLE_CUBE_ONLY
    using B_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, BType, false, LayoutMode::NONE, true>;
    AscendC::Matmul<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE, CFG_NORM> matmulObj;
#else
    using B_TYPE = AscendC::MatmulType<AscendC::TPosition::GM, CubeFormat::ND, BType, false, LayoutMode::NONE, true>;
    AscendC::Matmul<A_TYPE, B_TYPE, C_TYPE, BIAS_TYPE, CFG_IBSHARE_NORM> matmulObj;
#endif

private:
    /**
     * @brief  Calculate the gm offset based on the blockIdx.
     * @param  blockIdx: Current Core blockidx.
     * @param  offsetA: Gm offset of A matrix.
     * @param  offsetB: Gm offset of B matrix.
     * @param  offsetC: Gm offset of C matrix.
     * @param  offsetBias: Gm offset of Bias matrix.
     * @retval None
     */
    __aicore__ inline void CalcOffset(int32_t blockIdx, int32_t& offsetA, int32_t& offsetB, int32_t& offsetC,
                                      int32_t& offsetBias);

    AscendC::GlobalTensor<AType> aGlobal;
    AscendC::GlobalTensor<BType> bGlobal;
    AscendC::GlobalTensor<CType> cGlobal;
    AscendC::GlobalTensor<BiasType> biasGlobal;
    TCubeTiling tiling;
    int32_t mCoreIndex;
    int32_t nCoreIndex;
    bool isTransA{false};
    bool isTransB{false};
};

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulKernel<AType, BType, CType, BiasType>::Init(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c,
                                                                         const TCubeTiling& tiling, bool isTransA,
                                                                         bool isTransB)
{
    this->tiling = tiling;
    aGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ AType*>(a), tiling.M * tiling.Ka);
    bGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ BType*>(b), tiling.Kb * tiling.N);
    cGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ CType*>(c), tiling.M * tiling.N);
    biasGlobal.SetGlobalBuffer(reinterpret_cast<__gm__ BiasType*>(bias), tiling.N);

    int32_t offsetA = 0;
    int32_t offsetB = 0;
    int32_t offsetC = 0;
    int32_t offsetBias = 0;
    this->isTransA = isTransA;
    this->isTransB = isTransB;
    CalcOffset(AscendC::GetBlockIdx(), offsetA, offsetB, offsetC, offsetBias);
    aGlobal = aGlobal[offsetA];
    bGlobal = bGlobal[offsetB];
    cGlobal = cGlobal[offsetC];
    biasGlobal = biasGlobal[offsetBias];
    if (GetSysWorkSpacePtr() == nullptr) {
        return;
    }
}

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulKernel<AType, BType, CType, BiasType>::Process()
{
    if (AscendC::GetBlockIdx() >= tiling.usedCoreNum) {
        return;
    }
    matmulObj.SetTensorA(aGlobal, isTransA);
    matmulObj.SetTensorB(bGlobal, isTransB);
    if (tiling.isBias) {
        matmulObj.SetBias(biasGlobal);
    }
    matmulObj.IterateAll(cGlobal);
    matmulObj.End();
}

template <typename AType, typename BType, typename CType, typename BiasType>
__aicore__ inline void MatmulKernel<AType, BType, CType, BiasType>::CalcOffset(int32_t blockIdx, int32_t& offsetA,
                                                                               int32_t& offsetB, int32_t& offsetC,
                                                                               int32_t& offsetBias)
{
    const TCubeTiling& tiling = this->tiling;
    auto mSingleBlocks = (tiling.M + tiling.singleCoreM - 1) / tiling.singleCoreM; // split M into mSingleBlocks cores
    mCoreIndex = blockIdx % mSingleBlocks;
    nCoreIndex = blockIdx / mSingleBlocks;

    if (isTransA) {
        offsetA = mCoreIndex * tiling.singleCoreM;
    } else {
        offsetA = mCoreIndex * tiling.Ka * tiling.singleCoreM;
    }
    if (isTransB) {
        offsetB = nCoreIndex * tiling.Kb * tiling.singleCoreN;
    } else {
        offsetB = nCoreIndex * tiling.singleCoreN;
    }
    offsetC = mCoreIndex * tiling.N * tiling.singleCoreM + nCoreIndex * tiling.singleCoreN;
    offsetBias = nCoreIndex * tiling.singleCoreN;
}

/**
 * @brief  matmul_ibshareb_custom kernel function.
 * @param  a: A matrix gm addr.
 * @param  b: B matrix gm addr.
 * @param  bias: Bias matrix gm addr.
 * @param  c: C matrix gm addr.
 * @param  workspace: Temporary gm space addr required by matmul calc.
 * @param  tilingGm: Tiling data addr.
 * @retval None
 */
extern "C" __global__ __aicore__ void matmul_ibshareb_custom(GM_ADDR a, GM_ADDR b, GM_ADDR bias, GM_ADDR c,
                                                             GM_ADDR __kfc_workspace__ workspace, GM_ADDR tilingGm)
{
#ifdef ENABLE_CUBE_ONLY
    KERNEL_TASK_TYPE_DEFAULT(KERNEL_TYPE_AIC_ONLY);
#endif
    // prepare tiling
    TCubeTiling tiling;
    CopyTiling(&tiling, tilingGm);
    // define matmul kernel
    MatmulKernel<half, half, float, float> matmulKernel;
    AscendC::TPipe pipe;
    REGIST_MATMUL_OBJ(&pipe, GetSysWorkSpacePtr(), matmulKernel.matmulObj, &tiling);
    // init matmul kernel, isTransA=false, isTransB=false
    matmulKernel.Init(a, b, bias, c, tiling, false, false);
    // matmul kernel process
    matmulKernel.Process();
}

/**
 * @brief Generate matmul tiling.
 * @param testCaseParams: Testcase parameters.
 * @retval Generated Tiling data.
 */
TCubeTiling GenerateTiling(const MatmulCaseParams& testCaseParams)
{
    TCubeTiling tilingData;
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    if (ascendcPlatform == nullptr) {
        return tilingData;
    }
    matmul_tiling::MultiCoreMatmulTiling cubeTiling(*ascendcPlatform);
    uint32_t M = testCaseParams.m;
    uint32_t N = testCaseParams.n;
    uint32_t K = testCaseParams.k;
    bool hasBias = testCaseParams.hasBias;
    bool isAtrans = testCaseParams.isATrans;
    bool isBtrans = testCaseParams.isBTrans;

    cubeTiling.SetDim(USED_CORE_NUM);
    cubeTiling.SetAType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
                        matmul_tiling::DataType::DT_FLOAT16, isAtrans);
    cubeTiling.SetBType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
                        matmul_tiling::DataType::DT_FLOAT16, isBtrans);
    cubeTiling.SetCType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND, matmul_tiling::DataType::DT_FLOAT);
    cubeTiling.SetBiasType(matmul_tiling::TPosition::GM, matmul_tiling::CubeFormat::ND,
                           matmul_tiling::DataType::DT_FLOAT);
    cubeTiling.SetOrgShape(M, N, K);
    cubeTiling.SetShape(M, N, K);
    cubeTiling.EnableBias(hasBias);
    cubeTiling.SetBufferSpace(-1, -1, -1);
    if (cubeTiling.GetTiling(tilingData) == -1) {
        std::cout << "Generate tiling failed." << std::endl;
        return {};
    }
    return tilingData;
}

static size_t GetSysWorkSpaceSize()
{
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    return static_cast<size_t>(ascendcPlatform->GetLibApiWorkSpaceSize());
}

void MatmulOp(uint8_t* x1, uint8_t* x2, uint8_t* y, uint8_t* bias, int64_t m, int64_t n, int64_t k,
              void* stream = nullptr)
{
    // Init args
    uint8_t* workspaceDevice = nullptr;

    // Query workspace size
    size_t workspaceSize = GetSysWorkSpaceSize();

    // Allocate workspace on device
    aclrtMalloc((void**)&workspaceDevice, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);

    // Initialize kernel with arguments and workspace pointer
    uint8_t* tilingHost = nullptr;
    uint8_t* tilingDevice = nullptr;
    size_t tilingFileSize = sizeof(TCubeTiling);
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    MatmulCaseParams testCaseParams{
        static_cast<int32_t>(m), static_cast<int32_t>(n), static_cast<int32_t>(k), IS_BIAS, IS_A_TRANS, IS_B_TRANS};
    // Calculate Tiling
    const auto tilingData = GenerateTiling(testCaseParams);
    aclrtMallocHost((void**)(&tilingHost), tilingFileSize);
    aclrtMalloc((void**)&tilingDevice, tilingFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    aclrtMemcpy(tilingHost, tilingFileSize, &tilingData, tilingFileSize, ACL_MEMCPY_HOST_TO_HOST);
    aclrtMemcpy(tilingDevice, tilingFileSize, tilingHost, tilingFileSize, ACL_MEMCPY_HOST_TO_DEVICE);
    // Launch kernel
    #ifdef ENABLE_CUBE_ONLY
    matmul_ibshareb_custom<<<tilingData.usedCoreNum, nullptr, stream>>>(x1, x2, bias, y, workspaceDevice,
                                                                                    tilingDevice);
    #else
    matmul_ibshareb_custom<<<tilingData.usedCoreNum / MIX_RATIO, nullptr, stream>>>(x1, x2, bias, y, workspaceDevice,
                                                                                    tilingDevice);
    #endif
    

    aclrtFreeHost(tilingHost);
    aclrtFree(workspaceDevice);
    aclrtFree(tilingDevice);
}

void TestAclInit(aclrtContext& context, aclrtStream& stream, int64_t& deviceId)
{
    aclInit(nullptr);
    aclrtSetDevice(deviceId);
    aclrtCreateContext(&context, deviceId);
    aclrtCreateStream(&stream);
}

void TestAclDeInit(aclrtContext& context, aclrtStream& stream, int64_t& deviceId)
{
    aclrtDestroyStream(stream);
    aclrtDestroyContext(context);
    aclrtResetDevice(deviceId);
    aclFinalize();
}

void TestMatmul(int64_t m, int64_t n, int64_t k, const MatrixFileSize& matrixFileSize)
{
    size_t x1FileSize = matrixFileSize.x1FileSize;
    size_t x2FileSize = matrixFileSize.x2FileSize;
    size_t yFileSize = matrixFileSize.yFileSize;
    size_t biasFileSize = matrixFileSize.biasFileSize;

    aclrtContext context;
    aclrtStream stream = nullptr;
    int64_t deviceId = 0;
    TestAclInit(context, stream, deviceId);

    uint8_t* x1Host = nullptr;
    uint8_t* x1Device = nullptr;
    aclrtMallocHost((void**)(&x1Host), x1FileSize);
    aclrtMalloc((void**)&x1Device, x1FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x1_gm.bin", x1FileSize, x1Host, x1FileSize);
    aclrtMemcpy(x1Device, x1FileSize, x1Host, x1FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* x2Host = nullptr;
    uint8_t* x2Device = nullptr;
    aclrtMallocHost((void**)(&x2Host), x2FileSize);
    aclrtMalloc((void**)&x2Device, x2FileSize, ACL_MEM_MALLOC_HUGE_FIRST);
    ReadFile("./input/x2_gm.bin", x2FileSize, x2Host, x2FileSize);
    aclrtMemcpy(x2Device, x2FileSize, x2Host, x2FileSize, ACL_MEMCPY_HOST_TO_DEVICE);

    uint8_t* biasHost = nullptr;
    uint8_t* biasDevice = nullptr;
    if (IS_BIAS) {
        aclrtMallocHost((void**)(&biasHost), biasFileSize);
        aclrtMalloc((void**)&biasDevice, biasFileSize, ACL_MEM_MALLOC_HUGE_FIRST);
        ReadFile("./input/bias_gm.bin", biasFileSize, biasHost, biasFileSize);
        aclrtMemcpy(biasDevice, biasFileSize, biasHost, biasFileSize, ACL_MEMCPY_HOST_TO_DEVICE);
    }
    uint8_t* yHost = nullptr;
    uint8_t* yDevice = nullptr;
    aclrtMallocHost((void**)(&yHost), yFileSize);
    aclrtMalloc((void**)&yDevice, yFileSize, ACL_MEM_MALLOC_HUGE_FIRST);

    MatmulOp(x1Device, x2Device, yDevice, biasDevice, m, n, k, stream);
    aclrtSynchronizeStream(stream);

    aclrtMemcpy(yHost, yFileSize, yDevice, yFileSize, ACL_MEMCPY_DEVICE_TO_HOST);
    WriteFile("./output/output.bin", yHost, yFileSize);

    if (IS_BIAS) {
        aclrtFree(biasDevice);
        aclrtFreeHost(biasHost);
    }
    aclrtFree(x1Device);
    aclrtFreeHost(x1Host);
    aclrtFree(x2Device);
    aclrtFreeHost(x2Host);
    aclrtFree(yDevice);
    aclrtFreeHost(yHost);
    TestAclDeInit(context, stream, deviceId);
}

int32_t main(int32_t argc, const char* args[])
{
    int64_t inputParams[3] = {64, 256, 384};
    for (int32_t i = 1; i < argc && i < 4; ++i) { // 4 used for inputParams loop
        std::stringstream ss(args[i]);
        ss >> inputParams[i - 1];
    }
    auto ascendcPlatform = platform_ascendc::PlatformAscendCManager::GetInstance();
    int64_t M = inputParams[0];
    int64_t N = inputParams[1];
    int64_t K = inputParams[2];
    MatrixFileSize matrixFileSize;
    // uint16_t represent half
    matrixFileSize.x1FileSize = static_cast<size_t>(M * K) * sizeof(uint16_t);
    matrixFileSize.x2FileSize = static_cast<size_t>(K * N) * sizeof(uint16_t);
    matrixFileSize.yFileSize = static_cast<size_t>(M * N) * sizeof(float);
    matrixFileSize.biasFileSize = static_cast<size_t>(1 * N) * sizeof(float);
    TestMatmul(M, N, K, matrixFileSize);
    return 0;
}
